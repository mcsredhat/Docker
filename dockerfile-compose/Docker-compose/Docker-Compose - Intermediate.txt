# Docker Compose - Intermediate Level: Lesson 12

I'm excited to dive into optimizing Docker Compose workflows with you! This lesson covers several powerful techniques that can significantly improve how you develop and deploy containerized applications. Let's explore each concept in depth.

## Override Default Configurations with docker-compose.override.yml

Docker Compose follows a specific loading order for configuration files. By default, it first loads `docker-compose.yml` and then automatically looks for and merges in a file called `docker-compose.override.yml`.

This pattern allows you to:
- Keep your base configuration in `docker-compose.yml` (ideal for version control)
- Add environment-specific overrides in `docker-compose.override.yml` (which might not be committed to version control)

For example, your base file might look like:

```yaml
# docker-compose.yml
version: '3.8'
services:
  web:
    image: nginx:latest
    ports:
      - "80:80"
```

While your override file for development could be:

```yaml
# docker-compose.override.yml
version: '3.8'
services:
  web:
    volumes:
      - ./src:/usr/share/nginx/html
    environment:
      - DEBUG=true
```

When you run `docker-compose up`, both files are automatically merged. It's like having different layers of configuration that come together when needed.

## Using .env Files for Environment Variables

Environment variables are crucial for configuring services without hardcoding values. With Docker Compose, you can place these variables in a `.env` file in the same directory as your compose file:

```
# .env
DB_PASSWORD=securepassword
API_KEY=1234567890
ENVIRONMENT=development
```

You can then reference these variables in your compose files:

```yaml
services:
  database:
    image: postgres
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - ENVIRONMENT=${ENVIRONMENT:-production}  # Default if not set
```

This approach keeps sensitive information out of your compose files and makes it easier to deploy in different environments without changing the compose files themselves.

## Sharing Data Between Containers with Named Volumes

Named volumes create persistent storage areas that can be shared between containers:

```yaml
volumes:
  shared-data:  # This is the named volume

services:
  service1:
    volumes:
      - shared-data:/app/data
  
  service2:
    volumes:
      - shared-data:/app/shared
```

In this example, both `service1` and `service2` can read and write to the same data storage area, even though they're mounting it at different paths within their containers. Named volumes persist even after containers are removed, making them ideal for databases and other stateful applications.

## Debugging & Logging

Docker Compose provides several commands to help debug your multi-container applications:

- `docker-compose logs [service]` - View output from services
- `docker-compose ps` - List containers and their states
- `docker-compose events` - Get real-time events from containers

The logs command is particularly versatile:
```bash
# Follow logs in real-time with timestamps
docker-compose logs --follow --timestamps web

# Get only the last 100 lines
docker-compose logs --tail=100 database
```

These commands help you understand what's happening in your application without having to dig through individual container details.

## Using Profiles for Selective Service Startups

Profiles allow you to selectively start services based on different scenarios:

```yaml
services:
  app:
    image: myapp:latest
  
  database:
    image: postgres
  
  test-suite:
    image: myapp-tests
    profiles:
      - testing
  
  monitoring:
    image: grafana/grafana
    profiles:
      - monitoring
```

With this configuration:
- `docker-compose up` starts only `app` and `database`
- `docker-compose --profile testing up` additionally starts `test-suite`
- `docker-compose --profile monitoring up` additionally starts `monitoring`

This is perfect for separating development, testing, and production services within a single compose file.

## Live Development with Compose Watch

The `watch` feature (introduced in Docker Compose v2.22.0) enables automatic rebuilding and restarting of services when source files change:

```yaml
services:
  web:
    build: ./app
    watch:
      - action: rebuild
        path: ./app
        exclude:
          - node_modules/
```

This creates a smooth development workflow where:
1. You modify source code
2. Docker Compose detects the changes
3. The container is automatically rebuilt and restarted
4. You can immediately see your changes

## Hands-on Demo: Multiple Configurations with Override Files and Watch

Let's create a simple Flask application with different configurations for development and production.

First, let's set up our project structure:
```
project/
├── app/
│   ├── app.py
│   └── requirements.txt
├── docker-compose.yml
├── docker-compose.override.yml
└── docker-compose.prod.yml
```

The Flask application (`app.py`):
```python
from flask import Flask
import os

app = Flask(__name__)

@app.route('/')
def hello():
    env = os.environ.get('FLASK_ENV', 'unknown')
    return f'Hello from {env} environment!'

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=os.environ.get('FLASK_DEBUG', 'false').lower() == 'true')
```

Requirements (`requirements.txt`):
```
flask==2.0.1
```

Base configuration (`docker-compose.yml`):
```yaml
version: '3.8'
services:
  web:
    build: ./app
    image: flask-demo
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - FLASK_DEBUG=false
```

Development overrides (`docker-compose.override.yml`):
```yaml
version: '3.8'
services:
  web:
    volumes:
      - ./app:/app
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=true
    watch:
      - action: rebuild
        path: ./app
        exclude:
          - __pycache__/
```

Production configuration (`docker-compose.prod.yml`):
```yaml
version: '3.8'
services:
  web:
    restart: always
    environment:
      - FLASK_ENV=production
      - FLASK_DEBUG=false
```

Dockerfile (`app/Dockerfile`):
```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

### Running the Demo

For development (uses both `docker-compose.yml` and `docker-compose.override.yml` automatically):
```bash
docker-compose up
```

For production:
```bash
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

With the development setup, any changes to your Flask application will trigger an automatic rebuild, allowing you to see changes immediately without manually restarting containers.

## Mini Project: Production-Ready Laravel + MySQL Stack with Profiles

Let's build a complete Laravel application stack with different profiles for development, testing, and production.

Project structure:
```
laravel-project/
├── src/                   # Laravel source code
├── .env                   # Environment variables
├── docker-compose.yml
└── docker/
    ├── php/
    │   └── Dockerfile
    └── nginx/
        └── default.conf
```

Environment file (`.env`):
```
DB_PASSWORD=laravelpassword
APP_ENV=development
```

Main compose file (`docker-compose.yml`):

```yaml
version: '3.8'

services:
  # PHP Application
  app:
    build:
      context: ./docker/php
    volumes:
      - ./src:/var/www/html
    depends_on:
      - database
    environment:
      - DB_HOST=database
      - DB_PASSWORD=${DB_PASSWORD}
      - APP_ENV=${APP_ENV:-production}

  # Web Server
  web:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./src:/var/www/html
      - ./docker/nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - app

  # Database
  database:
    image: mysql:8.0
    volumes:
      - db-data:/var/lib/mysql
    environment:
      - MYSQL_DATABASE=laravel
      - MYSQL_ROOT_PASSWORD=${DB_PASSWORD}
    ports:
      - "3306:3306"

  # Redis (for caching)
  redis:
    image: redis:alpine
    volumes:
      - redis-data:/data

  # PHPMyAdmin - Dev only
  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    ports:
      - "8080:80"
    environment:
      - PMA_HOST=database
    depends_on:
      - database
    profiles:
      - dev

  # Testing tools
  test-runner:
    build:
      context: ./docker/php
    volumes:
      - ./src:/var/www/html
    depends_on:
      - database
    environment:
      - DB_HOST=database
      - DB_PASSWORD=${DB_PASSWORD}
      - APP_ENV=testing
    command: ["php", "artisan", "test"]
    profiles:
      - testing

  # Production monitoring
  prometheus:
    image: prom/prometheus
    volumes:
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    profiles:
      - monitoring

  grafana:
    image: grafana/grafana
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    profiles:
      - monitoring

volumes:
  db-data:
  redis-data:
  prometheus-data:
  grafana-data:
```

PHP Dockerfile (`docker/php/Dockerfile`):
```dockerfile
FROM php:8.1-fpm

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libpng-dev \
    libjpeg-dev \
    libfreetype6-dev \
    zip \
    unzip \
    git

# Install PHP extensions
RUN docker-php-ext-install pdo_mysql

# Get latest Composer
COPY --from=composer:latest /usr/bin/composer /usr/bin/composer

# Set working directory
WORKDIR /var/www/html

# Set recommended PHP.ini settings
RUN mv "$PHP_INI_DIR/php.ini-production" "$PHP_INI_DIR/php.ini"

CMD ["php-fpm"]
```

Nginx configuration (`docker/nginx/default.conf`):
```
server {
    listen 80;
    index index.php index.html;
    server_name localhost;
    error_log  /var/log/nginx/error.log;
    access_log /var/log/nginx/access.log;
    root /var/www/html/public;

    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }

    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass app:9000;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param PATH_INFO $fastcgi_path_info;
    }
}
```

### Running Different Profiles

Basic development environment:
```bash
docker-compose up
```

Development with PHPMyAdmin:
```bash
docker-compose --profile dev up
```

Running tests:
```bash
docker-compose --profile testing up test-runner
```

Production with monitoring:
```bash
docker-compose --profile monitoring up -d
```

This setup demonstrates:
1. A complete production-ready application stack
2. Separation of concerns using profiles
3. Environment-specific configurations
4. Persistent data storage with named volumes
5. Proper networking between containers

## Key Takeaways

1. **Configuration Management**: Use a combination of base compose files, override files, and environmental variables to maintain clean, flexible configurations across environments.

2. **Data Management**: Named volumes provide a clean way to persist and share data between containers without worrying about container lifecycles.

3. **Development Workflows**: Tools like `watch` can dramatically improve development speed by automating rebuild/restart cycles.

4. **Operational Efficiency**: Debugging tools and profiles help you maintain and troubleshoot complex application stacks efficiently.

5. **Production Readiness**: Proper configuration separation allows you to use the same compose files from development through testing and into production with minimal changes.

These advanced Docker Compose techniques help bridge the gap between simple container setups and full-scale container orchestration systems like Kubernetes, making it an excellent tool for both development and small to medium production deployments.

Would you like me to elaborate on any particular aspect of Docker Compose we've covered? Or perhaps you'd like to see how to implement a specific pattern for your own projects?

=================================================
# Docker Compose - Intermediate Level: Lesson 13

In this lesson, we'll explore how to enhance communication between containers and manage their dependencies effectively. These techniques will help you build more robust and secure containerized applications.

## Custom Networking

Docker Compose creates a default network for your services, but you can define custom networks with specific configurations. Custom networks provide better isolation, security, and control over how containers communicate.

Here's how to define custom networks in your compose file:

```yaml
networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true  # Not accessible from outside Docker

services:
  webapp:
    networks:
      - frontend
  
  api:
    networks:
      - frontend
      - backend
  
  database:
    networks:
      - backend
```

In this example, the `api` service can communicate with both `webapp` and `database`, but `webapp` cannot directly access `database` because they don't share a network. The `internal: true` flag makes the backend network inaccessible from outside Docker, adding an extra layer of security.

You can also create networks manually and reference them in your compose file:

```bash
docker network create --driver bridge --subnet 172.28.0.0/16 my-custom-network
```

Then in your compose file:

```yaml
networks:
  my-custom-network:
    external: true
```

This approach gives you more control over network configuration, including IP address ranges and subnet masks.

## Cross-Container Communication

Services in a Docker Compose setup can communicate with each other using their service names as hostnames. This built-in DNS resolution makes it easy for containers to find each other.

For example, if you have a web service that needs to connect to a database:

```yaml
services:
  web:
    environment:
      - DATABASE_URL=postgres://user:password@database:5432/dbname
  
  database:
    image: postgres
```

The web service can connect to the database using the hostname `database` instead of an IP address. This service discovery works across custom networks too, making it easy to build complex architectures.

You can also use Docker's DNS features to implement more advanced service discovery patterns:

```yaml
services:
  api:
    networks:
      backend:
        aliases:
          - api.internal
          - api-service.internal
```

This allows other services on the `backend` network to reach this service using either `api.internal` or `api-service.internal` as hostnames, which can be useful for migration or implementing virtual hosts.

## Using healthcheck for Service Dependencies

While Docker Compose's `depends_on` ensures services start in the right order, it doesn't guarantee they're ready to accept connections. This is where health checks become invaluable:

```yaml
services:
  web:
    depends_on:
      database:
        condition: service_healthy
  
  database:
    image: postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
```

In this example, the `web` service will start only after the `database` service passes its health check. The health check runs the `pg_isready` command every 5 seconds to verify if PostgreSQL is accepting connections.

Health checks have several configuration options:
- `test`: The command to run to check health
- `interval`: How often to run the check
- `timeout`: How long to wait for the check to complete
- `retries`: How many consecutive failures are needed to consider the container unhealthy
- `start_period`: Initial grace period during which failures don't count against `retries`

This mechanism helps prevent application errors when one service tries to connect to another that isn't ready yet.

## Automatic Service Restart

Docker Compose provides several restart policies to ensure your services recover from crashes or system reboots:

```yaml
services:
  database:
    restart: always
  
  cache:
    restart: unless-stopped
  
  worker:
    restart: on-failure
    restart_policy:
      max_attempts: 3
  
  dev-service:
    restart: "no"
```

These policies determine how Docker handles container failures:
- `always`: Always restart the container regardless of exit state (even after a Docker daemon restart)
- `unless-stopped`: Restart the container except when it was manually stopped
- `on-failure`: Restart only if the container exits with a non-zero status code
- `no`: Never automatically restart the container

For production environments, `unless-stopped` is often a good choice as it persists through Docker daemon restarts but respects manual interventions.

## Running Background Services

When you're working with multiple services, you often want to run them in the background. Docker Compose's detached mode is perfect for this:

```bash
docker-compose up -d
```

This starts all services in the background. You can still access logs when needed:

```bash
docker-compose logs -f service-name
```

The `-f` (follow) flag streams logs in real-time, similar to `tail -f`.

You can also bring up specific services in detached mode:

```bash
docker-compose up -d database cache
```

This is particularly useful for starting infrastructure services that other services depend on.

## Secure Communication

Security in container communication is critical, especially in production environments. Docker Compose provides several mechanisms to implement secure communication patterns.

### Managing Secrets

Docker has built-in support for secrets management:

```yaml
services:
  webapp:
    image: my-webapp
    secrets:
      - db_password
      - ssl_cert

secrets:
  db_password:
    file: ./secrets/db_password.txt
  ssl_cert:
    file: ./secrets/ssl_cert.pem
```

Secrets are mounted at `/run/secrets/<secret_name>` inside the container, keeping sensitive data out of environment variables or config files.

For development environments, you can also use environment variables with a secrets-like pattern:

```yaml
services:
  webapp:
    environment:
      - DB_PASSWORD_FILE=/run/secrets/db_password
    volumes:
      - ./dev-secrets:/run/secrets
```

### TLS Between Services

For production environments, you may want to encrypt traffic between services using TLS:

```yaml
services:
  api:
    volumes:
      - ./certs:/etc/certs
    environment:
      - CERT_PATH=/etc/certs/api.crt
      - KEY_PATH=/etc/certs/api.key
      - CLIENT_CA_PATH=/etc/certs/ca.crt
```

You'd then configure your application to use these certificates for both server and client authentication, implementing mutual TLS (mTLS) for secure service-to-service communication.

Docker also supports network encryption at the swarm level, but that's beyond the scope of Docker Compose and enters the realm of Docker Swarm orchestration.

## Hands-on Demo: Create a Robust Microservices Network with Health Checks and Secrets

Let's create a robust microservices architecture with proper networking, health checks, and secrets management. Our demo will include:
- An API gateway
- Two microservices
- A database
- A Redis cache

First, let's set up our project structure:

```
microservices-demo/
├── docker-compose.yml
├── gateway/
│   ├── Dockerfile
│   └── server.js
├── service-a/
│   ├── Dockerfile
│   └── server.js
├── service-b/
│   ├── Dockerfile
│   └── server.js
└── secrets/
    ├── db_password.txt
    └── api_key.txt
```

Now, let's create our Docker Compose configuration:

```yaml
# docker-compose.yml
version: '3.8'

services:
  gateway:
    build: ./gateway
    ports:
      - "3000:3000"
    networks:
      - frontend
      - services
    depends_on:
      redis:
        condition: service_healthy
      service-a:
        condition: service_healthy
      service-b:
        condition: service_healthy
    environment:
      - SERVICE_A_URL=http://service-a:4000
      - SERVICE_B_URL=http://service-b:5000
      - REDIS_URL=redis://redis:6379
    secrets:
      - api_key
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  service-a:
    build: ./service-a
    networks:
      - services
      - database
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB_HOST=postgres
      - DB_USER=postgres
      - DB_NAME=service_a
      - DB_PASSWORD_FILE=/run/secrets/db_password
    secrets:
      - db_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  service-b:
    build: ./service-b
    networks:
      - services
      - database
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB_HOST=postgres
      - DB_USER=postgres
      - DB_NAME=service_b
      - DB_PASSWORD_FILE=/run/secrets/db_password
    secrets:
      - db_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  postgres:
    image: postgres:14-alpine
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - database
    environment:
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
      - POSTGRES_USER=postgres
    secrets:
      - db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  redis:
    image: redis:6-alpine
    volumes:
      - redis-data:/data
    networks:
      - services
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

networks:
  frontend:
    driver: bridge
  services:
    driver: bridge
  database:
    driver: bridge
    internal: true

volumes:
  postgres-data:
  redis-data:

secrets:
  db_password:
    file: ./secrets/db_password.txt
  api_key:
    file: ./secrets/api_key.txt
```

Now let's create simplified Node.js services for our demo:

**Gateway (gateway/server.js):**
```javascript
const express = require('express');
const axios = require('axios');
const redis = require('redis');
const fs = require('fs');
const app = express();
const port = 3000;

// Read API key from Docker secret
const apiKey = fs.readFileSync('/run/secrets/api_key', 'utf8').trim();

// Create Redis client
const redisClient = redis.createClient({
  url: process.env.REDIS_URL
});

redisClient.connect().catch(console.error);

app.get('/health', (req, res) => {
  res.status(200).send('OK');
});

// Validate API key middleware
const validateApiKey = (req, res, next) => {
  const providedKey = req.header('X-API-Key');
  if (providedKey && providedKey === apiKey) {
    next();
  } else {
    res.status(401).send('Unauthorized');
  }
};

app.use(validateApiKey);

// Proxy requests to service-a
app.get('/service-a/:resource', async (req, res) => {
  try {
    // Check cache first
    const cacheKey = `service-a:${req.params.resource}`;
    const cachedData = await redisClient.get(cacheKey);
    
    if (cachedData) {
      return res.json(JSON.parse(cachedData));
    }
    
    // Forward request to service-a
    const response = await axios.get(`${process.env.SERVICE_A_URL}/${req.params.resource}`);
    
    // Cache the result
    await redisClient.set(cacheKey, JSON.stringify(response.data), {
      EX: 60 // Expire after 60 seconds
    });
    
    res.json(response.data);
  } catch (error) {
    res.status(500).send('Error proxying to service-a');
  }
});

// Proxy requests to service-b
app.get('/service-b/:resource', async (req, res) => {
  try {
    const response = await axios.get(`${process.env.SERVICE_B_URL}/${req.params.resource}`);
    res.json(response.data);
  } catch (error) {
    res.status(500).send('Error proxying to service-b');
  }
});

app.listen(port, () => {
  console.log(`Gateway listening at http://localhost:${port}`);
});
```

**Service A (service-a/server.js):**
```javascript
const express = require('express');
const { Pool } = require('pg');
const fs = require('fs');
const app = express();
const port = 4000;

// Read DB password from Docker secret
const dbPassword = fs.readFileSync('/run/secrets/db_password', 'utf8').trim();

// Configure PostgreSQL connection
const pool = new Pool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  database: process.env.DB_NAME,
  password: dbPassword,
  port: 5432,
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.status(200).send('OK');
  } catch (error) {
    res.status(500).send('Not healthy');
  }
});

// Sample data endpoint
app.get('/users', async (req, res) => {
  try {
    const result = await pool.query('SELECT * FROM users');
    res.json(result.rows);
  } catch (error) {
    res.status(500).send('Database error');
  }
});

// Initialize database on startup
const initDatabase = async () => {
  try {
    await pool.query(`
      CREATE TABLE IF NOT EXISTS users (
        id SERIAL PRIMARY KEY,
        name TEXT NOT NULL,
        email TEXT UNIQUE NOT NULL
      )
    `);
    
    // Insert sample data if table is empty
    const count = await pool.query('SELECT COUNT(*) FROM users');
    if (parseInt(count.rows[0].count) === 0) {
      await pool.query(`
        INSERT INTO users (name, email) VALUES
        ('Alice', 'alice@example.com'),
        ('Bob', 'bob@example.com')
      `);
    }
    
    console.log('Database initialized');
  } catch (error) {
    console.error('Database initialization failed:', error);
  }
};

app.listen(port, () => {
  console.log(`Service A listening at http://localhost:${port}`);
  initDatabase();
});
```

**Service B (service-b/server.js):**
```javascript
const express = require('express');
const { Pool } = require('pg');
const fs = require('fs');
const app = express();
const port = 5000;

// Read DB password from Docker secret
const dbPassword = fs.readFileSync('/run/secrets/db_password', 'utf8').trim();

// Configure PostgreSQL connection
const pool = new Pool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  database: process.env.DB_NAME,
  password: dbPassword,
  port: 5432,
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.status(200).send('OK');
  } catch (error) {
    res.status(500).send('Not healthy');
  }
});

// Sample data endpoint
app.get('/products', async (req, res) => {
  try {
    const result = await pool.query('SELECT * FROM products');
    res.json(result.rows);
  } catch (error) {
    res.status(500).send('Database error');
  }
});

// Initialize database on startup
const initDatabase = async () => {
  try {
    await pool.query(`
      CREATE TABLE IF NOT EXISTS products (
        id SERIAL PRIMARY KEY,
        name TEXT NOT NULL,
        price NUMERIC NOT NULL
      )
    `);
    
    // Insert sample data if table is empty
    const count = await pool.query('SELECT COUNT(*) FROM products');
    if (parseInt(count.rows[0].count) === 0) {
      await pool.query(`
        INSERT INTO products (name, price) VALUES
        ('Laptop', 1200),
        ('Phone', 800),
        ('Headphones', 150)
      `);
    }
    
    console.log('Database initialized');
  } catch (error) {
    console.error('Database initialization failed:', error);
  }
};

app.listen(port, () => {
  console.log(`Service B listening at http://localhost:${port}`);
  initDatabase();
});
```

**Dockerfiles for Each Service:**

For gateway, service-a, and service-b (with proper adjustments for each service):

```dockerfile
FROM node:16-alpine

WORKDIR /app

COPY package.json .
RUN npm install

COPY server.js .

EXPOSE 3000

CMD ["node", "server.js"]
```

**Secret Files:**

Create the following files with secure content:
- `secrets/db_password.txt` - A strong database password
- `secrets/api_key.txt` - A secure API key for accessing the gateway

### Running the Demo

1. Start the entire stack in detached mode:
   ```bash
   docker-compose up -d
   ```

2. Monitor the health status as services start up:
   ```bash
   docker-compose ps
   ```

3. Once all services are healthy, test the API gateway:
   ```bash
   curl -H "X-API-Key: $(cat secrets/api_key.txt)" http://localhost:3000/service-a/users
   curl -H "X-API-Key: $(cat secrets/api_key.txt)" http://localhost:3000/service-b/products
   ```

This demo demonstrates several important concepts:
- Multiple isolated networks for security
- Health checks for proper dependency management
- Secret management for sensitive information
- Caching with Redis
- API gateway pattern with authentication

## Mini Project: Deploy a Secure Messaging App (RabbitMQ + Node.js + MongoDB) with Secret Management

Now, let's build a secure messaging application using RabbitMQ for message queuing, Node.js for services, and MongoDB for data storage.

Project structure:
```
messaging-app/
├── docker-compose.yml
├── auth-service/
│   ├── Dockerfile
│   └── server.js
├── message-service/
│   ├── Dockerfile
│   └── server.js
├── worker-service/
│   ├── Dockerfile
│   └── worker.js
├── frontend/
│   ├── Dockerfile
│   └── server.js
└── secrets/
    ├── mongodb_root_password.txt
    ├── mongodb_user_password.txt
    ├── rabbitmq_password.txt
    └── jwt_secret.txt
```

Docker Compose configuration:

```yaml
version: '3.8'

services:
  auth-service:
    build: ./auth-service
    networks:
      - frontend-net
      - service-net
      - db-net
    depends_on:
      mongodb:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    environment:
      - MONGODB_URI=mongodb://app_user:${MONGODB_USER_PASSWORD}@mongodb:27017/messaging
      - RABBITMQ_URI=amqp://app_user:${RABBITMQ_PASSWORD}@rabbitmq:5672
      - JWT_SECRET_FILE=/run/secrets/jwt_secret
    secrets:
      - jwt_secret
      - mongodb_user_password
      - rabbitmq_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  message-service:
    build: ./message-service
    networks:
      - service-net
      - db-net
    depends_on:
      mongodb:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      auth-service:
        condition: service_healthy
    environment:
      - MONGODB_URI=mongodb://app_user:${MONGODB_USER_PASSWORD}@mongodb:27017/messaging
      - RABBITMQ_URI=amqp://app_user:${RABBITMQ_PASSWORD}@rabbitmq:5672
      - JWT_SECRET_FILE=/run/secrets/jwt_secret
    secrets:
      - jwt_secret
      - mongodb_user_password
      - rabbitmq_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  worker-service:
    build: ./worker-service
    networks:
      - service-net
      - db-net
    depends_on:
      mongodb:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    environment:
      - MONGODB_URI=mongodb://app_user:${MONGODB_USER_PASSWORD}@mongodb:27017/messaging
      - RABBITMQ_URI=amqp://app_user:${RABBITMQ_PASSWORD}@rabbitmq:5672
    secrets:
      - mongodb_user_password
      - rabbitmq_password
    healthcheck:
      test: ["CMD", "node", "/app/healthcheck.js"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "8080:8080"
    networks:
      - frontend-net
    depends_on:
      auth-service:
        condition: service_healthy
      message-service:
        condition: service_healthy
    environment:
      - AUTH_SERVICE_URL=http://auth-service:3000
      - MESSAGE_SERVICE_URL=http://message-service:3000
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  mongodb:
    image: mongo:5.0
    volumes:
      - mongodb-data:/data/db
      - ./mongo-init:/docker-entrypoint-initdb.d
    networks:
      - db-net
    environment:
      - MONGO_INITDB_ROOT_USERNAME=root
      - MONGO_INITDB_ROOT_PASSWORD_FILE=/run/secrets/mongodb_root_password
      - MONGO_INITDB_DATABASE=messaging
      - APP_USER=app_user
      - APP_USER_PASSWORD_FILE=/run/secrets/mongodb_user_password
    secrets:
      - mongodb_root_password
      - mongodb_user_password
    healthcheck:
      test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3.9-management
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
      - ./rabbitmq-init:/etc/rabbitmq/rabbitmq-init
    networks:
      - service-net
    environment:
      - RABBITMQ_DEFAULT_USER=app_user
      - RABBITMQ_DEFAULT_PASS_FILE=/run/secrets/rabbitmq_password
    secrets:
      - rabbitmq_password
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

networks:
  frontend-net:
    driver: bridge
  service-net:
    driver: bridge
  db-net:
    driver: bridge
    internal: true

volumes:
  mongodb-data:
  rabbitmq-data:

secrets:
  mongodb_root_password:
    file: ./secrets/mongodb_root_password.txt
  mongodb_user_password:
    file: ./secrets/mongodb_user_password.txt
  rabbitmq_password:
    file: ./secrets/rabbitmq_password.txt
  jwt_secret:
    file: ./secrets/jwt_secret.txt
```

Create an initialization script for MongoDB:

```bash
# mongo-init/create-user.sh
#!/bin/bash
set -e

# Wait for MongoDB to be ready
until mongo --eval "db.adminCommand('ping')" > /dev/null 2>&1; do
  echo "Waiting for MongoDB to start..."
  sleep 2
done

# Get the app user password from the Docker secret
APP_USER_PASSWORD=$(cat $APP_USER_PASSWORD_FILE)

# Create application user with appropriate permissions
mongo admin -u root -p $(cat $MONGO_INITDB_ROOT_PASSWORD_FILE) --eval "
  db = db.getSiblingDB('$MONGO_INITDB_DATABASE');
  db.createUser({
    user: '$APP_USER',
    pwd: '$APP_USER_PASSWORD',
    roles: [
      { role: 'readWrite', db: '$MONGO_INITDB_DATABASE' }
    ]
  });
  db.createCollection('messages');
  db.createCollection('users');
"

echo "MongoDB initialized successfully"
```

Let's create a simple RabbitMQ initialization script:

```bash
# rabbitmq-init/setup-queues.sh
#!/bin/bash
set -e

# Wait for RabbitMQ to be ready
until rabbitmqctl status > /dev/null 2>&1; do
  echo "Waiting for RabbitMQ to start..."
  sleep 2
done

# Create queues and exchanges
rabbitmqctl add_vhost messaging
rabbitmqctl set_permissions -p messaging $RABBITMQ_DEFAULT_USER ".*" ".*" ".*"

# Declare queues
rabbitmqadmin declare queue --vhost=messaging name=message_queue durable=true
rabbitmqadmin declare queue --vhost=messaging name=notification_queue durable=true

# Declare exchanges
rabbitmqadmin declare exchange --vhost=messaging name=message_exchange type=direct durable=true
rabbitmqadmin declare exchange --vhost=messaging name=notification_exchange type=fanout durable=true

# Bind queues to exchanges
rabbitmqadmin declare binding --vhost=messaging source=message_exchange destination=message_queue routing_key=message
rabbitmqadmin declare binding --vhost=messaging source=notification_exchange destination=notification_queue routing_key=notification

echo "RabbitMQ queues and exchanges initialized successfully"
```

For each service, we'll create a basic implementation:

**Auth Service (auth-service/server.js):**
```javascript
const express = require('express');
const jwt = require('jsonwebtoken');
const bcrypt = require('bcrypt');
const { MongoClient } = require('mongodb');
const amqp = require('amqplib');
const fs = require('fs');
const app = express();
const port = 3000;

app.use(express.json());

// Read secrets
const jwtSecret = fs.readFileSync('/run/secrets/jwt_secret', 'utf8').trim();
const mongoPassword = fs.readFileSync('/run/secrets/mongodb_user_password', 'utf8').trim();
const rabbitmqPassword = fs.readFileSync('/run/secrets/rabbitmq_password', 'utf8').trim();

// Replace password placeholders in connection strings
const mongodbUri = process.env.MONGODB_URI.replace('${MONGODB_USER_PASSWORD}', mongoPassword);
const rabbitmqUri = process.env.RABBITMQ_URI.replace('${RABBITMQ_PASSWORD}', rabbitmqPassword);

// MongoDB connection
let db;
MongoClient.connect(mongodbUri)
  .then(client => {
    db = client.db();
    console.log('Connected to MongoDB');
  })
  .catch(err => {
    console.error('Failed to connect to MongoDB:', err);
  });

// RabbitMQ connection
let channel;
async function connectRabbitMQ() {
  try {
    const connection = await amqp.connect(rabbitmqUri);
    channel = await connection.createChannel();
    console.log('Connected to RabbitMQ');
  } catch (err) {
    console.error('Failed to connect to RabbitMQ:', err);
    setTimeout(connectRabbitMQ, 5000);
  }
}
connectRabbitMQ();

// Health check endpoint
app.get('/health', (req, res) => {
  if (db && channel) {
    res.status(200).send('OK');
  } else {
    res.status(503).send('Service Unavailable');
  }
});

// Register endpoint
app.post('/register', async (req, res) => {
  try {
    const { username, password, email } = req.body;
    
    // Check if user exists
    const existingUser = await db.collection('users').findOne({ username });
    if (existingUser) {
      return res.status(409).json({ message: 'Username already exists' });
    }
    
    // Hash password
    const hashedPassword = await bcrypt.hash(password, 10);
    
    // Create user
    const result = await db.collection('users').insertOne({
      username,
      password: hashedPassword,
      email,
      createdAt: new Date()
    });
    
    // Publish event to RabbitMQ
    await channel.publish('notification_exchange', 'notification', Buffer.from(JSON.stringify({
      type: 'user_registered',
      userId: result.insertedId.toString(),
      timestamp: new Date()
    })));
    
    res.status(201).json({ message: 'User registered successfully' });
  } catch (error) {
    console.error('Registration error:', error);
    res.status(500).json({ message: 'Internal server error' });
  }
});

// Login endpoint
app.post('/login', async (req, res) => {
  try {
    const { username, password } = req.body;
    
    // Find user
    const user = await db.collection('users').findOne({ username });
    if (!user) {
      return res.status(401).json({ message: 'Invalid credentials' });
    }
    
    // Check password
    const passwordMatch = await bcrypt.compare(password, user.password);
    if (!passwordMatch) {
      return res.status(401).json({ message: 'Invalid credentials' });
    }
    
    // Generate JWT
    const token = jwt.sign(
      { userId: user._id.toString(), username: user.username },
      jwtSecret,
      { expiresIn: '24h' }
    );
    
    res.json({ token });
  } catch (error) {
    console.error('Login error:', error);
    res.status(500).json({ message: 'Internal server error' });
  }
});

// Validate token middleware (to be used by other services)
app.post('/validate-token', (req, res) => {
  const token = req.body.token;
================================================================
# Docker Compose - Intermediate Level: Lesson 14

In this lesson, we'll explore how to effectively manage multi-container applications in production environments. As containerized applications move from development to production, different challenges arise related to deployment, configuration management, performance monitoring, and service orchestration.

## Deploying with docker-compose up -d --build

When deploying updates to a containerized application in production, you need a streamlined approach to rebuild images and restart services with minimal downtime. The `docker-compose up -d --build` command combines several operations into one efficient workflow:

```bash
docker-compose up -d --build
```

This command:
1. Builds or rebuilds all service images that have a `build` directive in your compose file
2. Starts all services defined in your compose file
3. Runs in detached mode (`-d`), meaning containers run in the background
4. Creates or recreates only containers whose configuration or image has changed

This approach is particularly useful when you've made changes to your application code and need to deploy those changes without disrupting other services. Docker Compose is smart enough to only rebuild and restart what's necessary.

For a more controlled deployment process, especially in production environments, you might want to split these steps:

```bash
# Build or rebuild images
docker-compose build

# Bring everything up in detached mode
docker-compose up -d
```

This separation allows you to verify that the build process completed successfully before actually deploying the changes.

## Handling Configuration Updates

When you need to update configurations or pull new versions of images, a common pattern is:

```bash
# Pull the latest versions of all images
docker-compose pull

# Apply the updates
docker-compose up -d
```

This sequence ensures you have the latest versions of all your images before redeploying. Docker Compose will only recreate containers for services whose images have changed.

For more controlled updates, especially when updating critical services, you might want to update services individually:

```bash
# Pull the latest version of a specific service
docker-compose pull web-service

# Update just that service
docker-compose up -d web-service
```

When handling configuration changes stored in environment files, you might need to reload the compose file to pick up these changes:

```bash
# After updating your .env file
docker-compose down
docker-compose up -d
```

Or for more targeted updates that don't require a full restart:

```bash
docker-compose up -d --force-recreate --no-deps web-service
```

This recreation ensures the service picks up new environment variables without stopping dependent services.

## Managing Logs & Monitoring Performance

### Resource Limits

In production environments, it's crucial to set resource limits to prevent any single container from consuming all available system resources:

```yaml
services:
  web-app:
    image: web-app:latest
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
```

For older versions of Docker Compose (pre-version 3), you can use the direct properties:

```yaml
services:
  web-app:
    image: web-app:latest
    mem_limit: 512m
    cpu_limit: 0.5
```

These limits ensure that:
- The container can't use more than 50% of a CPU core
- The container can't consume more than 512MB of memory
- The container has a guaranteed reservation of 25% of a CPU core and 256MB of memory

Setting appropriate resource limits helps prevent resource contention and improves overall system stability, especially when running multiple services on the same host.

### Log Management

For proper log management in production, you can configure Docker's logging drivers:

```yaml
services:
  web-app:
    image: web-app:latest
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

This configuration:
- Uses the JSON file logging driver
- Limits log files to 10MB in size
- Keeps a maximum of 3 log files before rotating

For more advanced setups, you might want to send logs to centralized logging systems like ELK (Elasticsearch, Logstash, Kibana) or use the `fluentd` or `syslog` drivers.

## Using depends_on vs. Health Checks for Stability

Both `depends_on` and health checks help manage service dependencies, but they serve different purposes:

### depends_on

The `depends_on` option ensures services start in the correct order:

```yaml
services:
  web-app:
    image: web-app:latest
    depends_on:
      - database
      - redis
  
  database:
    image: postgres:latest
  
  redis:
    image: redis:latest
```

This configuration guarantees that:
- The `database` and `redis` services start before the `web-app` service
- When stopping services, Docker Compose stops them in reverse order

While `depends_on` handles startup order, it doesn't verify if services are actually ready to accept connections—it only ensures they've started. This can lead to application errors if your app tries to connect to a database that isn't yet ready to accept connections.

### Health Checks

Health checks provide a more robust way to handle dependencies by verifying that services are actually ready:

```yaml
services:
  web-app:
    image: web-app:latest
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
  
  database:
    image: postgres:latest
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  redis:
    image: redis:latest
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
```

With this approach:
- The `web-app` service only starts after both `database` and `redis` pass their health checks
- Each service defines what it means to be "healthy" through a test command
- Services are given time to initialize with the `start_period` option

In production environments, combining `depends_on` with health checks provides the most reliable approach to service orchestration.

## Optimizing Startup Order of Services

Beyond the basic `depends_on` and health checks, complex applications often need more nuanced control over startup processes:

### Staged Startup

For applications with many services, a staged startup approach can reduce resource contention:

```yaml
services:
  # Infrastructure tier
  database:
    image: postgres:latest
    healthcheck: # ...
  
  redis:
    image: redis:latest
    healthcheck: # ...
  
  # Application tier
  api-service:
    image: api-service:latest
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
  
  auth-service:
    image: auth-service:latest
    depends_on:
      database:
        condition: service_healthy
  
  # Frontend tier
  web-app:
    image: web-app:latest
    depends_on:
      api-service:
        condition: service_healthy
      auth-service:
        condition: service_healthy
```

This tiered approach:
- Groups services into logical tiers (infrastructure, application, frontend)
- Ensures each tier starts in the correct order
- Reduces the "thundering herd" problem when all services try to start simultaneously

### Startup Scripts

For more complex initialization, you can use entrypoint scripts to handle application-specific startup logic:

```yaml
services:
  api-service:
    image: api-service:latest
    entrypoint: ["/app/wait-for-it.sh", "database:5432", "--", "/app/startup.sh"]
```

These scripts can:
- Check for dependent services using tools like `wait-for-it.sh` or `dockerize`
- Perform database migrations
- Load initial data
- Set up runtime configurations

This approach moves complex startup logic out of Docker Compose and into your application's domain, making it more maintainable and adaptable.

## Scaling Services and Orchestration Prep

As your application grows, you may need to scale certain services to handle increased load:

### Basic Scaling

Docker Compose supports basic horizontal scaling with the `--scale` flag:

```bash
docker-compose up -d --scale worker=3 --scale web-app=2
```

This command:
- Starts 3 instances of the `worker` service
- Starts 2 instances of the `web-app` service
- Maintains a single instance of any other services

For this to work properly, your services must be designed to run in parallel without port conflicts:

```yaml
services:
  web-app:
    image: web-app:latest
    ports:
      - "8080-8085:8080"  # Port range allows multiple instances
```

### Preparing for Orchestration

While Docker Compose can handle basic scaling, production applications often migrate to full orchestration platforms like Docker Swarm or Kubernetes. You can prepare your Compose files for this transition:

```yaml
version: '3.8'

services:
  web-app:
    image: web-app:latest
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.role == worker]
```

The `deploy` section includes configurations that will be used by Docker Swarm (but ignored by Docker Compose):
- `replicas`: The number of containers to run
- `update_config`: How updates should be rolled out
- `restart_policy`: When containers should be restarted
- `placement`: Where containers should be deployed in a swarm

This dual-compatibility approach allows you to use the same configuration files across development (Docker Compose) and production (Docker Swarm) environments.

## Hands-on Demo: Monitoring Dashboard with Prometheus + Grafana

Let's set up a monitoring dashboard for your containerized applications using Prometheus and Grafana, complete with resource constraints to ensure stability.

First, create a project directory:

```bash
mkdir docker-monitoring
cd docker-monitoring
```

### Step 1: Create the Compose File

Create a `docker-compose.yml` file:

```yaml
version: '3.8'

services:
  # Application to monitor (example web service)
  web-app:
    image: nginx:alpine
    ports:
      - "8080:80"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/status.conf:/etc/nginx/conf.d/status.conf:ro
    depends_on:
      - prometheus
    networks:
      - monitoring_network

  # Node exporter to collect host metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
    networks:
      - monitoring_network

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin_password
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - monitoring_network
    depends_on:
      prometheus:
        condition: service_healthy

networks:
  monitoring_network:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
```

### Step 2: Configure Prometheus

Create the Prometheus configuration file:

```bash
mkdir -p prometheus
```

Create `prometheus/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'nginx'
    static_configs:
      - targets: ['web-app:80']
    metrics_path: /metrics
```

### Step 3: Configure Nginx with the Prometheus Exporter

Create Nginx configuration files:

```bash
mkdir -p nginx
```

Create `nginx/nginx.conf`:

```nginx
user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    keepalive_timeout  65;

    include /etc/nginx/conf.d/*.conf;
}
```

Create `nginx/status.conf`:

```nginx
server {
    listen 80;
    server_name localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }

    location /metrics {
        stub_status on;
        access_log off;
        allow all;
    }
}
```

### Step 4: Configure Grafana Dashboards

Set up Grafana provisioning:

```bash
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards
```

Create `grafana/provisioning/datasources/datasource.yml`:

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
```

Create `grafana/provisioning/dashboards/dashboard.yml`:

```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

### Step 5: Launch the Monitoring Stack

Start everything with:

```bash
docker-compose up -d
```

Access the monitoring interfaces:
- Grafana: http://localhost:3000 (login with admin/admin_password)
- Prometheus: http://localhost:9090

### Step 6: Import a Dashboard

In Grafana, navigate to Dashboards > Import and use dashboard ID `1860` for Node Exporter Full.

This setup provides:
- Host metrics collection via Node Exporter
- NGINX metrics collection
- Visualization with Grafana
- Resource constraints to ensure stability
- Health checks to manage dependencies correctly

## Mini Project: Video Streaming App with Scaling and Monitoring

Let's create a video streaming application that can scale to handle multiple requests and includes monitoring.

### Project Structure

```
video-streaming-app/
├── docker-compose.yml
├── nginx/
│   ├── nginx.conf
│   └── default.conf
├── api/
│   ├── Dockerfile
│   ├── package.json
│   └── server.js
├── encoder/
│   ├── Dockerfile
│   └── encoder.js
├── monitoring/
│   ├── prometheus.yml
│   └── grafana/
│       ├── datasources/
│       │   └── datasource.yml
│       └── dashboards/
│           └── dashboard.yml
└── frontend/
    ├── Dockerfile
    ├── package.json
    └── src/
        ├── App.js
        └── ...
```

### Docker Compose Configuration

```yaml
version: '3.8'

services:
  # Nginx for serving videos and load balancing
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
      - video_data:/var/www/videos:ro
    networks:
      - frontend_net
      - streaming_net
    depends_on:
      - api
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # API for video metadata and upload
  api:
    build: ./api
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/video_streaming
      - PORT=3000
    volumes:
      - video_data:/app/videos
    networks:
      - streaming_net
      - backend_net
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Video Encoder Service
  encoder:
    build: ./encoder
    volumes:
      - video_data:/app/videos
    environment:
      - REDIS_URL=redis://redis:6379
    networks:
      - backend_net
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "node", "/app/healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Frontend React App
  frontend:
    build: ./frontend
    ports:
      - "3001:80"
    networks:
      - frontend_net
    depends_on:
      - api
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # MongoDB for metadata storage
  mongodb:
    image: mongo:4.4
    volumes:
      - mongodb_data:/data/db
    networks:
      - backend_net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s

  # Redis for job queue
  redis:
    image: redis:6-alpine
    networks:
      - backend_net
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - monitoring_net
      - backend_net
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    volumes:
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin_secret
    networks:
      - monitoring_net
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 512M

networks:
  frontend_net:
    driver: bridge
  streaming_net:
    driver: bridge
  backend_net:
    driver: bridge
    internal: true
  monitoring_net:
    driver: bridge

volumes:
  video_data:
  mongodb_data:
  prometheus_data:
  grafana_data:
```

### API Service Implementation (api/server.js)

```javascript
const express = require('express');
const mongoose = require('mongoose');
const multer = require('multer');
const path = require('path');
const Redis = require('ioredis');
const promClient = require('prom-client');
const fs = require('fs');
const app = express();
const port = process.env.PORT || 3000;

// Set up Prometheus metrics
const register = new promClient.Registry();
promClient.collectDefaultMetrics({ register });

// Custom metrics
const httpRequestDurationMs = new promClient.Histogram({
  name: 'http_request_duration_ms',
  help: 'Duration of HTTP requests in ms',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [10, 50, 100, 500, 1000, 5000]
});
register.registerMetric(httpRequestDurationMs);

const videoUploads = new promClient.Counter({
  name: 'video_uploads_total',
  help: 'Total number of video uploads'
});
register.registerMetric(videoUploads);

// Connect to MongoDB
mongoose.connect(process.env.MONGODB_URI, {
  useNewUrlParser: true,
  useUnifiedTopology: true,
});
const db = mongoose.connection;
db.on('error', console.error.bind(console, 'MongoDB connection error:'));

// Connect to Redis
const redisClient = new Redis(process.env.REDIS_URL || 'redis://redis:6379');

// Define Video schema
const videoSchema = new mongoose.Schema({
  title: String,
  description: String,
  filename: String,
  originalFilename: String,
  encoding: {
    type: String,
    enum: ['pending', 'processing', 'completed', 'failed'],
    default: 'pending'
  },
  createdAt: {
    type: Date,
    default: Date.now
  }
});
const Video = mongoose.model('Video', videoSchema);

// Configure multer for file uploads
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    const uploadsDir = path.join(__dirname, 'videos', 'uploads');
    if (!fs.existsSync(uploadsDir)) {
      fs.mkdirSync(uploadsDir, { recursive: true });
    }
    cb(null, uploadsDir);
  },
  filename: (req, file, cb) => {
    cb(null, Date.now() + path.extname(file.originalname));
  }
});
const upload = multer({ storage });

// Middleware
app.use(express.json());
app.use('/videos', express.static(path.join(__dirname, 'videos')));

// Metrics middleware
app.use((req, res, next) => {
  const start = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - start;
    httpRequestDurationMs
      .labels(req.method, req.route?.path || req.path, res.statusCode.toString())
      .observe(duration);
  });
  next();
});

// Health check endpoint
app.get('/health', (req, res) => {
  const mongoStatus = mongoose.connection.readyState === 1; // 1 = connected
  
  redisClient.ping((err, result) => {
    const redisStatus = !err && result === 'PONG';
    
    if (mongoStatus && redisStatus) {
      res.status(200).json({ status: 'healthy' });
    } else {
      res.status(503).json({
        status: 'unhealthy',
        mongo: mongoStatus ? 'connected' : 'disconnected',
        redis: redisStatus ? 'connected' : 'disconnected'
      });
    }
  });
});

// Metrics endpoint for Prometheus
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

// List videos endpoint
app.get('/api/videos', async (req, res) => {
  try {
    const videos = await Video.find().sort({ createdAt: -1 });
    res.json(videos);
  } catch (error) {
    console.error('Error fetching videos:', error);
    res.status(500).json({ error: 'Error fetching videos' });
  }
});

// Get single video endpoint
app.get('/api/videos/:id', async (req, res) => {
  try {
    const video = await Video.findById(req.params.id);
    if (!video) {
      return res.status(404).json({ error: 'Video not found' });
    }
    res.json(video);
  } catch (error) {
    console.error('Error fetching video:', error);
    res.status(500).json({ error: 'Error fetching video' });
  }
});

// Upload video endpoint
app.post('/api/videos/upload', upload.single('video'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No video file provided' });
    }

    const video = new Video({
      title: req.body.title || 'Untitled',
      description: req.body.description || '',
      filename: req.file.filename,
      originalFilename: req.file.originalname,
    });

    await video.save();
    
    // Increment upload counter
    videoUploads.inc();

    // Add job to encoding queue
    await redisClient.rpush('encoding_queue', JSON.stringify({
      videoId: video._id,
      filePath: req.file.path
    }));

    res.status(201).json(video);
  } catch (error) {
    console.error('Error uploading video:', error);
    res.status(500).json({ error: 'Error uploading video' });
  }
});

// Start the server
app.listen(port, () => {
  console.log(`API server running on port ${port}`);
});
```

### Encoder Service Implementation (encoder/encoder.js)

```javascript
const Redis = require('ioredis');
const mongoose = require('mongoose');
const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');
const promClient = require('prom-client');

// Setup Prometheus metrics
const register = new promClient.Registry();
promClient.collectDefaultMetrics({ register });

const encodingsProcessed = new promClient.Counter({
  name: 'video_encodings_processed_total',
  help: 'Total number of video encodings processed',
  labelNames: ['status']
});
register.registerMetric(encodingsProcessed);

const encodingDurationSeconds = new promClient.Histogram({
  name: 'video_encoding_duration_seconds',
  help: 'Duration of video encoding in seconds',
  buckets: [10, 30, 60, 120, 300, 600, 1800]
});
register.registerMetric(encodingDurationSeconds);

// Connect to MongoDB
mongoose.connect(process.env.MONGODB_URI || 'mongodb://mongodb:27017/video_streaming', {
  useNewUrlParser: true,
  useUnifiedTopology: true,
});
const db = mongoose.connection;
db.on('error', console.error.bind(console, 'MongoDB connection error:'));

// Define Video schema (matching the API service)
const videoSchema = new mongoose.Schema({
  title: String,
  description: String,
  filename: String,
  originalFilename: String,
  encoding: {
    type: String,
    enum: ['pending', 'processing', 'completed', 'failed'],
    default: 'pending'
  },
  createdAt: {
    type: Date,
    default: Date.now
  }
});
const Video = mongoose.model('Video', videoSchema);

// Create Redis client
const redisClient = new Redis(process.env.REDIS_URL || 'redis://redis:6379');
const subscriber = new Redis(process.env.REDIS_URL || 'redis://redis:6379');

// Express server for health and metrics
const express = require('express');
const app = express();
const port = process.env.PORT || 3001;

app.get('/health', (req, res) => {
  const mongoStatus = mongoose.connection.readyState === 1;
  
  redisClient.ping((err, result) => {
    const redisStatus = !err && result === 'PONG';
    
    if (mongoStatus && redisStatus) {
      res.status(200).json({ status: 'healthy' });
    } else {
      res.status(503).json({
        status: 'unhealthy',
        mongo: mongoStatus ? 'connected' : 'disconnected',
        redis: redisStatus ? 'connected' : 'disconnected'
      });
    }
  });
});

app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

app.listen(port, () => {
  console.log(`Encoder metrics server running on port ${port}`);
});

// Create output directories
const baseDir = path.join(__dirname, 'videos');
const outputDir = path.join(baseDir, 'encoded');
if (!
