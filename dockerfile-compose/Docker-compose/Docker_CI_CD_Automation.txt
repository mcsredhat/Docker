# Docker Compose - Advanced Level: CI/CD & Automation

Continuous Integration and Continuous Deployment (CI/CD) processes are essential for modern software development, and Docker plays a crucial role in making these workflows reliable and consistent. This lesson explores how to integrate Docker Compose into CI/CD pipelines and automate various aspects of container management.

## Integrating Docker Compose with Jenkins

Jenkins is one of the most popular CI/CD tools, and it works exceptionally well with Docker Compose for automated build and deployment processes.

### Setting Up Jenkins with Docker

There are two main approaches to integrating Jenkins with Docker:

1. **Docker-in-Docker (DinD)**: Running Docker inside a Jenkins container
2. **Docker-outside-of-Docker (DooD)**: Mounting the host's Docker socket to a Jenkins container

The DooD approach is generally preferred for its simplicity and better performance:

```yaml
version: '3.8'

services:
  jenkins:
    image: jenkins/jenkins:lts
    privileged: true
    user: root
    ports:
      - "8080:8080"
      - "50000:50000"
    volumes:
      - jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker

volumes:
  jenkins_home:
```

This setup gives Jenkins access to the host's Docker daemon, allowing it to create and manage containers.

### Creating a Jenkins Pipeline for Docker Compose

Jenkins Pipelines can be defined in a `Jenkinsfile` that automates the entire build and deployment process:

```groovy
pipeline {
    agent any
    
    environment {
        DOCKER_COMPOSE_VERSION = '2.15.1'
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Build') {
            steps {
                sh 'docker-compose build'
            }
        }
        
        stage('Test') {
            steps {
                sh 'docker-compose run --rm app npm test'
            }
        }
        
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                sh 'docker-compose down'
                sh 'docker-compose up -d'
            }
        }
    }
    
    post {
        always {
            sh 'docker-compose logs'
            sh 'docker-compose ps'
        }
        failure {
            sh 'docker-compose down'
        }
    }
}
```

This pipeline:
1. Checks out code from the repository
2. Builds the Docker images defined in the docker-compose.yml
3. Runs tests in a temporary container
4. Deploys the application (only when on the main branch)
5. Logs container status after each run
6. Cleans up containers if the pipeline fails

### Jenkins Agents with Docker

For more complex CI/CD setups, you can use Jenkins agents that already have Docker and Docker Compose installed:

```yaml
pipeline {
    agent {
        docker {
            image 'docker/compose:latest'
            args '-v /var/run/docker.sock:/var/run/docker.sock'
        }
    }
    
    stages {
        // Pipeline stages...
    }
}
```

This approach eliminates the need to install Docker tools in your Jenkins master.

## Automating Builds & Deployments

Automation reduces manual errors and accelerates delivery cycles. Let's explore several approaches to automating Docker workflows.

### Automated Builds with Docker Hub

Docker Hub offers automated build services that can trigger builds when you push to your Git repository:

1. Connect your Docker Hub account to GitHub or Bitbucket
2. Create a new automated build repository
3. Configure build rules for different branches/tags
4. Add a webhook to trigger builds automatically

### Using GitHub Actions for Docker Compose

GitHub Actions provides a more modern approach to CI/CD with Docker:

```yaml
# .github/workflows/docker-compose.yml
name: Docker Compose CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1
    
    - name: Build the Docker Compose stack
      run: docker-compose build
    
    - name: Run tests
      run: docker-compose run --rm app npm test
    
    - name: Login to DockerHub
      if: github.ref == 'refs/heads/main'
      uses: docker/login-action@v1
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
    
    - name: Push images
      if: github.ref == 'refs/heads/main'
      run: docker-compose push
```

This workflow:
1. Triggers on pushes to the main branch or pull requests
2. Sets up Docker Buildx for efficient builds
3. Builds the Docker Compose services
4. Runs tests with the built images
5. Authenticates with Docker Hub (only on the main branch)
6. Pushes the images to Docker Hub (only on the main branch)

### Implementing Blue-Green Deployments

Blue-green deployment is a technique that maintains two identical production environments, only one of which handles production traffic at any time:

```bash
#!/bin/bash
# blue-green-deploy.sh

# Determine current active environment
if docker network inspect blue-network &>/dev/null; then
  CURRENT="blue"
  NEW="green"
else
  CURRENT="green"
  NEW="blue"
fi

echo "Current environment: $CURRENT, deploying to: $NEW"

# Deploy to the new environment
docker-compose -f docker-compose.$NEW.yml build
docker-compose -f docker-compose.$NEW.yml up -d

# Wait for new environment to be ready
echo "Waiting for $NEW environment to be healthy..."
for i in {1..30}; do
  if curl -f http://localhost:8${NEW: -1}00/health; then
    break
  fi
  sleep 2
done

# Switch traffic to new environment
echo "Switching traffic to $NEW environment..."
docker-compose -f docker-compose.proxy.yml stop proxy
sed -i "s/BACKEND=$CURRENT/BACKEND=$NEW/" .env
docker-compose -f docker-compose.proxy.yml up -d proxy

# Wait and verify
sleep 5
echo "Checking new environment..."
curl -f http://localhost/health

# Stop old environment if everything is OK
if [ $? -eq 0 ]; then
  echo "Stopping $CURRENT environment..."
  docker-compose -f docker-compose.$CURRENT.yml down
  echo "Deployment completed successfully!"
else
  echo "Deployment failed! Switching back to $CURRENT environment..."
  sed -i "s/BACKEND=$NEW/BACKEND=$CURRENT/" .env
  docker-compose -f docker-compose.proxy.yml up -d proxy
  docker-compose -f docker-compose.$NEW.yml down
  exit 1
fi
```

This script:
1. Identifies the currently active environment
2. Deploys the new version to the inactive environment
3. Waits for the new environment to be healthy
4. Switches traffic to the new environment by updating a proxy configuration
5. Stops the old environment if the switchover is successful
6. Rolls back if there are any issues

## Using Docker Secrets for Secure Configurations

Docker Secrets provide a secure way to manage sensitive information in container environments.

### Docker Secrets in Compose Files

For Docker Compose in swarm mode, you can define and use secrets:

```yaml
version: '3.8'

services:
  web:
    image: myapp:latest
    secrets:
      - db_password
      - ssl_cert
    environment:
      - DB_PASSWORD_FILE=/run/secrets/db_password

secrets:
  db_password:
    file: ./secrets/db_password.txt
  ssl_cert:
    file: ./secrets/ssl_cert.pem
```

Secrets are mounted as files in the container's `/run/secrets/` directory.

### Simulating Secrets in Development

For local development (without Swarm), you can simulate secrets:

```yaml
version: '3.8'

services:
  web:
    image: myapp:latest
    volumes:
      - ./dev-secrets/db_password:/run/secrets/db_password
    environment:
      - DB_PASSWORD_FILE=/run/secrets/db_password
```

### Using External Secret Managers

For production environments, consider integrating with dedicated secret management services:

1. **HashiCorp Vault**: For comprehensive secret management
2. **AWS Secrets Manager** or **Azure Key Vault**: For cloud-native applications
3. **Kubernetes Secrets**: If running on Kubernetes with Docker images

An example using Vault with Docker Compose:

```yaml
version: '3.8'

services:
  app:
    image: myapp:latest
    environment:
      - VAULT_ADDR=https://vault.example.com:8200
      - VAULT_TOKEN_FILE=/run/secrets/vault_token
    volumes:
      - ./vault-token:/run/secrets/vault_token:ro
    entrypoint: ["./wait-for-secrets.sh", "node", "app.js"]
```

The `wait-for-secrets.sh` script would fetch secrets from Vault before starting the application.

## Docker Buildx for Multi-Platform Images

Docker Buildx enables building images for multiple platforms from a single build process.

### Setting Up Buildx for Multi-Platform Builds

```bash
# Create a new builder instance
docker buildx create --name mybuilder --use

# Check available platforms
docker buildx inspect --bootstrap
```

### Configuring Multi-Platform Builds in Docker Compose

```yaml
# docker-compose.build.yml
version: '3.8'

services:
  app:
    build:
      context: ./app
      dockerfile: Dockerfile
      platforms:
        - linux/amd64
        - linux/arm64
```

Running the build:

```bash
docker buildx bake -f docker-compose.build.yml --push
```

### Creating a CI Pipeline for Multi-Platform Images

Here's a GitHub Actions workflow for multi-platform builds:

```yaml
name: Multi-Platform Docker Build

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Docker meta
        id: meta
        uses: docker/metadata-action@v3
        with:
          images: username/app
          tags: |
            type=semver,pattern={{version}}
            type=ref,event=branch
      
      - name: Build and push
        uses: docker/build-push-action@v2
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
```

This workflow:
1. Sets up QEMU for emulating different architectures
2. Configures Docker Buildx
3. Determines appropriate tags based on Git metadata
4. Builds and pushes multi-platform images

## Automated Testing with Compose in CI

Docker Compose simplifies running tests in CI environments by providing consistent testing environments.

### Setting Up Test Environments

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  app:
    build: .
    environment:
      - NODE_ENV=test
      - DATABASE_URL=postgres://postgres:postgres@db:5432/testdb
    depends_on:
      - db
    command: npm test
  
  db:
    image: postgres:13-alpine
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=testdb
    tmpfs: 
      - /var/lib/postgresql/data  # Use in-memory storage for tests
```

### Running Tests in CI

```bash
# Start all the dependencies
docker-compose -f docker-compose.test.yml up -d db

# Run the tests and get the exit code
docker-compose -f docker-compose.test.yml run --rm app

# Store the exit code
EXIT_CODE=$?

# Clean up
docker-compose -f docker-compose.test.yml down -v

# Exit with the same code as the tests
exit $EXIT_CODE
```

### Parallel Test Execution

For larger test suites, you might want to run tests in parallel:

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test-unit:
    build: .
    environment:
      - NODE_ENV=test
    command: npm run test:unit
  
  test-integration:
    build: .
    environment:
      - NODE_ENV=test
      - DATABASE_URL=postgres://postgres:postgres@db:5432/testdb
    depends_on:
      - db
    command: npm run test:integration
  
  test-e2e:
    build: .
    environment:
      - NODE_ENV=test
      - BASE_URL=http://app:3000
    depends_on:
      - app
      - db
    command: npm run test:e2e
  
  app:
    build: .
    environment:
      - NODE_ENV=test
      - DATABASE_URL=postgres://postgres:postgres@db:5432/testdb
    depends_on:
      - db
  
  db:
    image: postgres:13-alpine
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=testdb
    tmpfs: 
      - /var/lib/postgresql/data
```

Running parallel tests in CI:

```bash
docker-compose -f docker-compose.test.yml up -d db app
docker-compose -f docker-compose.test.yml up --exit-code-from test-unit test-unit
UNIT_EXIT=$?
docker-compose -f docker-compose.test.yml up --exit-code-from test-integration test-integration
INTEGRATION_EXIT=$?
docker-compose -f docker-compose.test.yml up --exit-code-from test-e2e test-e2e
E2E_EXIT=$?
docker-compose -f docker-compose.test.yml down -v

# If any test suite failed, fail the build
[ $UNIT_EXIT -eq 0 ] && [ $INTEGRATION_EXIT -eq 0 ] && [ $E2E_EXIT -eq 0 ]
```

## Hands-on Demo: Create a CI/CD Pipeline using Jenkins & Docker

Let's create a complete CI/CD pipeline for a Node.js application using Jenkins and Docker Compose.

### Project Structure

```
node-app/
├── .dockerignore
├── Dockerfile
├── Jenkinsfile
├── docker-compose.yml
├── docker-compose.test.yml
├── package.json
├── src/
│   ├── app.js
│   └── server.js
└── test/
    ├── app.test.js
    └── server.test.js
```

### Application Files

**package.json**:
```json
{
  "name": "node-app",
  "version": "1.0.0",
  "main": "src/server.js",
  "scripts": {
    "start": "node src/server.js",
    "test": "jest --coverage",
    "lint": "eslint src/"
  },
  "dependencies": {
    "express": "^4.17.1"
  },
  "devDependencies": {
    "eslint": "^7.32.0",
    "jest": "^27.2.0",
    "supertest": "^6.1.6"
  }
}
```

**src/app.js**:
```javascript
const express = require('express');

const app = express();

app.get('/', (req, res) => {
  res.json({ message: 'Hello, World!' });
});

app.get('/health', (req, res) => {
  res.status(200).send('OK');
});

module.exports = app;
```

**src/server.js**:
```javascript
const app = require('./app');

const PORT = process.env.PORT || 3000;

app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

**test/app.test.js**:
```javascript
const request = require('supertest');
const app = require('../src/app');

describe('App', () => {
  it('should respond with a message for the root path', async () => {
    const response = await request(app).get('/');
    expect(response.status).toBe(200);
    expect(response.body).toEqual({ message: 'Hello, World!' });
  });

  it('should respond to health check', async () => {
    const response = await request(app).get('/health');
    expect(response.status).toBe(200);
    expect(response.text).toBe('OK');
  });
});
```

### Docker Configuration

**Dockerfile**:
```dockerfile
FROM node:14-alpine as base

WORKDIR /app

COPY package*.json ./

FROM base as development
RUN npm install
COPY . .
CMD ["npm", "start"]

FROM base as test
RUN npm install
COPY . .
CMD ["npm", "test"]

FROM base as production
RUN npm ci --only=production
COPY . .
CMD ["npm", "start"]
```

**.dockerignore**:
```
node_modules/
npm-debug.log
coverage/
.git/
.gitignore
.env
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  app:
    build:
      context: .
      target: production
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
    restart: always
```

**docker-compose.test.yml**:
```yaml
version: '3.8'

services:
  app:
    build:
      context: .
      target: test
    environment:
      - NODE_ENV=test
    command: npm test
```

### Jenkins Pipeline Configuration

**Jenkinsfile**:
```groovy
pipeline {
    agent any
    
    environment {
        DOCKER_IMAGE = "node-app:${env.BUILD_ID}"
        COMPOSE_PROJECT_NAME = "${env.JOB_NAME}-${env.BUILD_ID}"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Build') {
            steps {
                sh 'docker-compose build'
            }
        }
        
        stage('Lint') {
            steps {
                sh 'docker-compose run --rm app npm run lint'
            }
        }
        
        stage('Test') {
            steps {
                sh 'docker-compose -f docker-compose.test.yml up --exit-code-from app'
            }
            post {
                always {
                    sh 'docker-compose -f docker-compose.test.yml down -v'
                }
            }
        }
        
        stage('Tag and Push') {
            when {
                branch 'main'
            }
            steps {
                script {
                    withCredentials([string(credentialsId: 'docker-hub-token', variable: 'DOCKER_HUB_TOKEN')]) {
                        sh 'docker login -u username -p ${DOCKER_HUB_TOKEN}'
                        sh 'docker tag node-app:${BUILD_ID} username/node-app:latest'
                        sh 'docker tag node-app:${BUILD_ID} username/node-app:${BUILD_ID}'
                        sh 'docker push username/node-app:latest'
                        sh 'docker push username/node-app:${BUILD_ID}'
                    }
                }
            }
        }
        
        stage('Deploy to Staging') {
            when {
                branch 'main'
            }
            steps {
                sshagent(['staging-server']) {
                    sh '''
                        ssh user@staging-server "cd /opt/node-app && \
                        docker-compose pull && \
                        docker-compose down && \
                        docker-compose up -d"
                    '''
                }
            }
        }
        
        stage('Integration Tests on Staging') {
            when {
                branch 'main'
            }
            steps {
                sh 'curl -f http://staging-server:3000/health || exit 1'
                // Add more integration tests here
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                input message: 'Deploy to production?'
                sshagent(['production-server']) {
                    sh '''
                        ssh user@production-server "cd /opt/node-app && \
                        docker-compose pull && \
                        docker-compose down && \
                        docker-compose up -d"
                    '''
                }
            }
        }
    }
    
    post {
        always {
            sh 'docker-compose down -v || true'
            sh 'docker rmi ${DOCKER_IMAGE} || true'
            cleanWs()
        }
        success {
            echo 'Pipeline succeeded!'
        }
        failure {
            echo 'Pipeline failed!'
        }
    }
}
```

### Setting Up Jenkins

1. Create a Jenkins instance with Docker installed:
   ```bash
   docker run -d -p 8080:8080 -p 50000:50000 \
     -v jenkins_home:/var/jenkins_home \
     -v /var/run/docker.sock:/var/run/docker.sock \
     -v /usr/bin/docker:/usr/bin/docker \
     --name jenkins jenkins/jenkins:lts
   ```

2. Configure Jenkins with necessary plugins:
   - Docker Pipeline
   - Pipeline
   - Git
   - SSH Agent

3. Add credentials:
   - Docker Hub token
   - SSH keys for staging and production servers

4. Create a Jenkins Pipeline job pointing to your Git repository

The pipeline will:
1. Check out the code from the repository
2. Build the Docker image
3. Run linting and tests
4. Push the image to Docker Hub (if on the main branch)
5. Deploy to staging automatically (if on the main branch)
6. Run integration tests against the staging environment
7. Wait for manual approval before deploying to production
8. Clean up resources when finished

## Mini Project: Automate Docker Builds & Deployments for a Full-Stack App

Let's create a CI/CD pipeline for a full-stack application consisting of a React frontend, a Node.js API, and a MongoDB database.

### Project Structure

```
fullstack-app/
├── .github/
│   └── workflows/
│       └── ci-cd.yml
├── docker-compose.yml
├── docker-compose.prod.yml
├── docker-compose.test.yml
├── frontend/
│   ├── Dockerfile
│   ├── package.json
│   └── src/
├── backend/
│   ├── Dockerfile
│   ├── package.json
│   └── src/
└── scripts/
    ├── deploy.sh
    └── rollback.sh
```

### Docker Compose Configuration

**docker-compose.yml** (Development):
```yaml
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      target: development
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - REACT_APP_API_URL=http://localhost:5000
    depends_on:
      - backend

  backend:
    build:
      context: ./backend
      target: development
    ports:
      - "5000:5000"
    volumes:
      - ./backend:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - MONGO_URI=mongodb://mongo:27017/app
    depends_on:
      - mongo

  mongo:
    image: mongo:4.4
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db

volumes:
  mongo_data:
```

**docker-compose.test.yml**:
```yaml
version: '3.8'

services:
  frontend-test:
    build:
      context: ./frontend
      target: test
    environment:
      - CI=true
    command: npm test

  backend-test:
    build:
      context: ./backend
      target: test
    environment:
      - NODE_ENV=test
      - MONGO_URI=mongodb://mongo:27017/test
    depends_on:
      - mongo
    command: npm test

  mongo:
    image: mongo:4.4
    tmpfs:
      - /data/db
```

**docker-compose.prod.yml**:
```yaml
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      target: production
    restart: always
    
  backend:
    build:
      context: ./backend
      target: production
    restart: always
    environment:
      - NODE_ENV=production
      - MONGO_URI=mongodb://mongo:27017/app
    depends_on:
      - mongo

  mongo:
    image: mongo:4.4
    volumes:
      - mongo_data:/data/db
    restart: always

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    restart: always

volumes:
  mongo_data:
```

### Frontend Dockerfile

```dockerfile
FROM node:14-alpine as base

WORKDIR /app

COPY package*.json ./

FROM base as development
RUN npm install
COPY . .
CMD ["npm", "start"]

FROM base as test
RUN npm install
COPY . .
CMD ["npm", "test"]

FROM base as build
RUN npm install
COPY . .
RUN npm run build

FROM nginx:alpine as production
COPY --from=build /app/build /usr/share/nginx/html
COPY nginx/default.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

### Backend Dockerfile

```dockerfile
FROM node:14-alpine as base

WORKDIR /app

COPY package*.json ./

FROM base as development
RUN npm install
COPY . .
CMD ["npm", "run", "dev"]

FROM base as test
RUN npm install
COPY . .
CMD ["npm", "test"]

FROM base as production
RUN npm ci --only=production
COPY . .
CMD ["npm", "start"]
```

### GitHub Actions Workflow

**.github/workflows/ci-cd.yml**:
```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Run frontend tests
      run: docker-compose -f docker-compose.test.yml up --exit-code-from frontend-test frontend-test
    
    - name: Run backend tests
      run: docker-compose -f docker-compose.test.yml up --exit-code-from backend-test backend-test
    
    - name: Clean up
      run: docker-compose -f docker-compose.test.yml down -v
  
  build-and-push:
    needs: test
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set environment variables
      run: |
        if [[ $GITHUB_REF == refs/heads/main ]]; then
          echo "ENV_NAME=production" >> $GITHUB_ENV
        else
          echo "ENV_NAME=staging" >> $GITHUB_ENV
        fi
        echo "IMAGE_TAG=$(echo $GITHUB_SHA | cut -c1-7)" >> $GITHUB_ENV
    
    - name: Login to DockerHub
      uses: docker/login-action@v1
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
    
    - name: Build and push frontend image
      uses: docker/build-push-action@v2
      with:
        context: ./frontend
        target: production
        push: true
        tags: |
          username/fullstack-frontend:${{ env.IMAGE_TAG }}
          username/fullstack-frontend:${{ env.ENV_NAME }}
    
    - name: Build and push backend image
      uses: docker/build-push-action@v2
      with:
        context: ./backend
        target: production
        push: true
        tags: |
          username/fullstack-backend:${{ env.IMAGE_TAG }}
          username/fullstack-backend:${{ env.ENV_NAME }}
    
    - name: Prepare deployment files
      run: |
        mkdir -p deploy
        cp docker-compose.prod.yml deploy/docker-compose.yml
        cp scripts/deploy.sh deploy/
        cp scripts/rollback.sh deploy/
        
        # Replace image tags in the docker-compose file
        sed -i "s|username/fullstack-frontend:latest|username/fullstack-frontend:${{ env.IMAGE_TAG }}|g" deploy/docker-compose.yml
        sed -i "s|username/fullstack-backend:latest|username/fullstack-backend:${{ env.IMAGE_TAG }}|g" deploy/docker-compose.yml
        
        # Create .env file
        echo "TAG=${{ env.IMAGE_TAG }}" > deploy/.env
        echo "ENV=${{ env.ENV_NAME }}" >> deploy/.env
    
    - name: Archive deployment files
      uses: actions/upload-artifact@v2
      with:
        name: deploy-files
        path: deploy/
  
  deploy-staging:
    needs: build-and-push
    if: github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    
    steps:
    - name: Download deployment files
      uses: actions/download-artifact@v2
      with:
        name: deploy-files
        path: deploy
    
    - name: Make deploy script executable
      run: chmod +x deploy/deploy.sh
    
    - name: Deploy to Staging
      uses: appleboy/ssh-action@master
      with:
        host: ${{ secrets.STAGING_HOST }}
        username: ${{ secrets.STAGING_USER }}
        key: ${{ secrets.STAGING_SSH_KEY }}
        script_stop: true
        script: |
          mkdir -p ~/app
          rm -rf ~/app/*
        
    - name: Copy deployment files
      uses: appleboy/scp-action@master
      with:
        host: ${{ secrets.STAGING_HOST }}
        username: ${{ secrets.STAGING_USER }}
        key: ${{ secrets.STAGING_SSH_KEY }}
        source: "deploy/*"
        target: "~/app"
        strip_components: 1
    
    - name: Execute deployment
      uses: appleboy/ssh-action@master
      with:
        host: ${{ secrets.STAGING_HOST }}
        username: ${{ secrets.STAGING_USER }}
        key: ${{ secrets.STAGING_SSH_KEY }}
        script: |
          cd ~/app
          ./deploy.sh
  
  deploy-production:
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    
    environment: production
    
    steps:
    - name: Download deployment files
      uses: actions/download-artifact@v2
      with:
        name: