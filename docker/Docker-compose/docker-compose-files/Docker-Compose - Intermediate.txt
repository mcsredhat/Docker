# Docker Compose - Intermediate Level: Lesson 12

I'm excited to dive into optimizing Docker Compose workflows with you! This lesson covers several powerful techniques that can significantly improve how you develop and deploy containerized applications. Let's explore each concept in depth.

## Override Default Configurations with docker-compose.override.yml

Docker Compose follows a specific loading order for configuration files. By default, it first loads `docker-compose.yml` and then automatically looks for and merges in a file called `docker-compose.override.yml`.

This pattern allows you to:
- Keep your base configuration in `docker-compose.yml` (ideal for version control)
- Add environment-specific overrides in `docker-compose.override.yml` (which might not be committed to version control)

For example, your base file might look like:

```yaml
# docker-compose.yml
services:
  web:
    image: nginx:latest
    ports:
      - "80:80"
```

While your override file for development could be:

```yaml
# docker-compose.override.yml
services:
  web:
    volumes:
      - ./src:/usr/share/nginx/html
    environment:
      - DEBUG=true
```

When you run `docker compose up`, both files are automatically merged. It's like having different layers of configuration that come together when needed.

## Using .env Files for Environment Variables

Environment variables are crucial for configuring services without hardcoding values. With Docker Compose, you can place these variables in a `.env` file in the same directory as your compose file:

```
# .env
DB_PASSWORD=securepassword
API_KEY=1234567890
ENVIRONMENT=development
```

You can then reference these variables in your compose files:

```yaml
services:
  database:
    image: postgres
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - ENVIRONMENT=${ENVIRONMENT:-production}  # Default if not set
```

This approach keeps sensitive information out of your compose files and makes it easier to deploy in different environments without changing the compose files themselves.

## Sharing Data Between Containers with Named Volumes

Named volumes create persistent storage areas that can be shared between containers:

```yaml
volumes:
  shared-data:  # This is the named volume

services:
  service1:
    volumes:
      - shared-data:/app/data
  
  service2:
    volumes:
      - shared-data:/app/shared
```

In this example, both `service1` and `service2` can read and write to the same data storage area, even though they're mounting it at different paths within their containers. Named volumes persist even after containers are removed, making them ideal for databases and other stateful applications.

## Debugging & Logging

Docker Compose provides several commands to help debug your multi-container applications:

- `docker compose logs [service]` - View output from services
- `docker compose ps` - List containers and their states
- `docker compose events` - Get real-time events from containers

The logs command is particularly versatile:
```bash
# Follow logs in real-time with timestamps
docker compose logs --follow --timestamps web

# Get only the last 100 lines
docker compose logs --tail=100 database
```

These commands help you understand what's happening in your application without having to dig through individual container details.

## Using Profiles for Selective Service Startups

Profiles allow you to selectively start services based on different scenarios:

```yaml
services:
  app:
    image: myapp:latest
  
  database:
    image: postgres
  
  test-suite:
    image: myapp-tests
    profiles:
      - testing
  
  monitoring:
    image: grafana/grafana
    profiles:
      - monitoring
```

With this configuration:
- `docker compose up` starts only `app` and `database`
- `docker compose --profile testing up` additionally starts `test-suite`
- `docker compose --profile monitoring up` additionally starts `monitoring`

This is perfect for separating development, testing, and production services within a single compose file.
###########################################
## Live Development with Compose Watch

The `watch` feature (introduced in Docker Compose v2.22.0) enables automatic rebuilding and restarting of services when source files change:

```yaml
services:
  web:
    build: ./app
    watch:
      - action: rebuild
        path: ./app
        exclude:
          - node_modules/
```

This creates a smooth development workflow where:
1. You modify source code
2. Docker Compose detects the changes
3. The container is automatically rebuilt and restarted
4. You can immediately see your changes

## Hands-on Demo: Multiple Configurations with Override Files and Watch

Let's create a simple Flask application with different configurations for development and production.

First, let's set up our project structure:
```
project/
├── app/
│   ├── app.py
│   ├── requirements.txt
│   └── Dockerfile
├── docker-compose.yml
├── docker-compose.override.yml
└── docker-compose.prod.yml
```

The Flask application (`app/app.py`):
```python
from flask import Flask
import os

app = Flask(__name__)

@app.route('/')
def hello():
    env = os.environ.get('FLASK_ENV', 'unknown')
    return f'Hello from {env} environment!'

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=os.environ.get('FLASK_DEBUG', 'false').lower() == 'true')
```

Requirements (`app/requirements.txt`):
```
flask==3.0.0
```

Dockerfile (`app/Dockerfile`):
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

Base configuration (`docker-compose.yml`):
```yaml
services:
  web:
    build: ./app
    image: flask-demo
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - FLASK_DEBUG=false
```

Development overrides (`docker-compose.override.yml`):
```yaml
services:
  web:
    volumes:
      - ./app:/app
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=true
    develop:
      watch:
        - action: rebuild
          path: ./app
          exclude:
            - __pycache__/
```

Production configuration (`docker-compose.prod.yml`):
```yaml
services:
  web:
    restart: always
    environment:
      - FLASK_ENV=production
      - FLASK_DEBUG=false
```

### Running the Demo

For development (uses both `docker-compose.yml` and `docker-compose.override.yml` automatically):
```bash
docker compose up
```

For production:
```bash
docker compose -f docker-compose.yml -f docker-compose.prod.yml up
```

With the development setup, any changes to your Flask application will trigger an automatic rebuild, allowing you to see changes immediately without manually restarting containers.

## Mini Project: Production-Ready Laravel + MySQL Stack with Profiles

Let's build a complete Laravel application stack with different profiles for development, testing, and production.

Project structure:
```
laravel-project/
├── src/                   # Laravel source code
├── .env                   # Environment variables
├── docker-compose.yml
└── docker/
    ├── php/
    │   └── Dockerfile
    └── nginx/
        └── default.conf
```

Environment file (`.env`):
```
DB_PASSWORD=laravelpassword
APP_ENV=development
```

Main compose file (`docker-compose.yml`):

```yaml
services:
  # PHP Application
  app:
    build:
      context: ./docker/php
    volumes:
      - ./src:/var/www/html
    depends_on:
      - database
    environment:
      - DB_HOST=database
      - DB_PASSWORD=${DB_PASSWORD}
      - APP_ENV=${APP_ENV:-production}

  # Web Server
  web:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./src:/var/www/html
      - ./docker/nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - app

  # Database
  database:
    image: mysql:8.0
    volumes:
      - db-data:/var/lib/mysql
    environment:
      - MYSQL_DATABASE=laravel
      - MYSQL_ROOT_PASSWORD=${DB_PASSWORD}
    ports:
      - "3306:3306"

  # Redis (for caching)
  redis:
    image: redis:alpine
    volumes:
      - redis-data:/data

  # PHPMyAdmin - Dev only
  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    ports:
      - "8080:80"
    environment:
      - PMA_HOST=database
    depends_on:
      - database
    profiles:
      - dev

  # Testing tools
  test-runner:
    build:
      context: ./docker/php
    volumes:
      - ./src:/var/www/html
    depends_on:
      - database
    environment:
      - DB_HOST=database
      - DB_PASSWORD=${DB_PASSWORD}
      - APP_ENV=testing
    command: ["php", "artisan", "test"]
    profiles:
      - testing

  # Production monitoring
  prometheus:
    image: prom/prometheus
    volumes:
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    profiles:
      - monitoring

  grafana:
    image: grafana/grafana
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    profiles:
      - monitoring

volumes:
  db-data:
  redis-data:
  prometheus-data:
  grafana-data:
```

PHP Dockerfile (`docker/php/Dockerfile`):
```dockerfile
FROM php:8.2-fpm

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libpng-dev \
    libjpeg-dev \
    libfreetype6-dev \
    zip \
    unzip \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install PHP extensions
RUN docker-php-ext-install pdo_mysql

# Get latest Composer
COPY --from=composer:latest /usr/bin/composer /usr/bin/composer

# Set working directory
WORKDIR /var/www/html

# Set recommended PHP.ini settings
RUN mv "$PHP_INI_DIR/php.ini-production" "$PHP_INI_DIR/php.ini"

CMD ["php-fpm"]
```

Nginx configuration (`docker/nginx/default.conf`):
```
server {
    listen 80;
    index index.php index.html;
    server_name localhost;
    error_log  /var/log/nginx/error.log;
    access_log /var/log/nginx/access.log;
    root /var/www/html/public;

    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }

    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass app:9000;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param PATH_INFO $fastcgi_path_info;
    }
}
```

### Running Different Profiles

Basic development environment:
```bash
docker compose up
```

Development with PHPMyAdmin:
```bash
docker compose --profile dev up
```

Running tests:
```bash
docker compose --profile testing up test-runner
```

Production with monitoring:
```bash
docker compose --profile monitoring up -d
```

This setup demonstrates:
1. A complete production-ready application stack
2. Separation of concerns using profiles
3. Environment-specific configurations
4. Persistent data storage with named volumes
5. Proper networking between containers

## Key Takeaways

1. **Configuration Management**: Use a combination of base compose files, override files, and environment variables to maintain clean, flexible configurations across environments.

2. **Data Management**: Named volumes provide a clean way to persist and share data between containers without worrying about container lifecycles.

3. **Development Workflows**: Tools like `watch` can dramatically improve development speed by automating rebuild/restart cycles.

4. **Operational Efficiency**: Debugging tools and profiles help you maintain and troubleshoot complex application stacks efficiently.

5. **Production Readiness**: Proper configuration separation allows you to use the same compose files from development through testing and into production with minimal changes.

These advanced Docker Compose techniques help bridge the gap between simple container setups and full-scale container orchestration systems like Kubernetes, making it an excellent tool for both development and small to medium production deployments.

Would you like me to elaborate on any particular aspect of Docker Compose we've covered? Or perhaps you'd like to see how to implement a specific pattern for your own projects?

=================================================
# Docker Compose - Intermediate Level: Lesson 13

In this lesson, we'll explore how to enhance communication between containers and manage their dependencies effectively. These techniques will help you build more robust and secure containerized applications.

## Custom Networking

Docker Compose creates a default network for your services, but you can define custom networks with specific configurations. Custom networks provide better isolation, security, and control over how containers communicate.

Here's how to define custom networks in your compose file:

```yaml
networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true  # Not accessible from outside Docker

services:
  webapp:
    networks:
      - frontend
  
  api:
    networks:
      - frontend
      - backend
  
  database:
    networks:
      - backend
```

In this example, the `api` service can communicate with both `webapp` and `database`, but `webapp` cannot directly access `database` because they don't share a network. The `internal: true` flag makes the backend network inaccessible from outside Docker, adding an extra layer of security.

You can also create networks manually and reference them in your compose file:

```bash
docker network create --driver bridge --subnet 172.28.0.0/16 my-custom-network
```

Then in your compose file:

```yaml
networks:
  my-custom-network:
    external: true
```

This approach gives you more control over network configuration, including IP address ranges and subnet masks.

## Cross-Container Communication

Services in a Docker Compose setup can communicate with each other using their service names as hostnames. This built-in DNS resolution makes it easy for containers to find each other.

For example, if you have a web service that needs to connect to a database:

```yaml
services:
  web:
    environment:
      - DATABASE_URL=postgres://user:password@database:5432/dbname
  
  database:
    image: postgres
```

The web service can connect to the database using the hostname `database` instead of an IP address. This service discovery works across custom networks too, making it easy to build complex architectures.

You can also use Docker's DNS features to implement more advanced service discovery patterns:

```yaml
services:
  api:
    networks:
      backend:
        aliases:
          - api.internal
          - api-service.internal
```

This allows other services on the `backend` network to reach this service using either `api.internal` or `api-service.internal` as hostnames, which can be useful for migration or implementing virtual hosts.

## Using healthcheck for Service Dependencies

While Docker Compose's `depends_on` ensures services start in the right order, it doesn't guarantee they're ready to accept connections. This is where health checks become invaluable:

```yaml
services:
  web:
    depends_on:
      database:
        condition: service_healthy
  
  database:
    image: postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
```

In this example, the `web` service will start only after the `database` service passes its health check. The health check runs the `pg_isready` command every 5 seconds to verify if PostgreSQL is accepting connections.

Health checks have several configuration options:
- `test`: The command to run to check health
- `interval`: How often to run the check
- `timeout`: How long to wait for the check to complete
- `retries`: How many consecutive failures are needed to consider the container unhealthy
- `start_period`: Initial grace period during which failures don't count against `retries`

This mechanism helps prevent application errors when one service tries to connect to another that isn't ready yet.

## Automatic Service Restart

Docker Compose provides several restart policies to ensure your services recover from crashes or system reboots:

```yaml
services:
  database:
    restart: always
  
  cache:
    restart: unless-stopped
  
  worker:
    restart: on-failure:3  # Restart on failure with max 3 attempts
  
  dev-service:
    restart: "no"
```

These policies determine how Docker handles container failures:
- `always`: Always restart the container regardless of exit state (even after a Docker daemon restart)
- `unless-stopped`: Restart the container except when it was manually stopped
- `on-failure`: Restart only if the container exits with a non-zero status code
- `no`: Never automatically restart the container

For production environments, `unless-stopped` is often a good choice as it persists through Docker daemon restarts but respects manual interventions.

## Running Background Services

When you're working with multiple services, you often want to run them in the background. Docker Compose's detached mode is perfect for this:

```bash
docker compose up -d
```

This starts all services in the background. You can still access logs when needed:

```bash
docker compose logs -f service-name
```

The `-f` (follow) flag streams logs in real-time, similar to `tail -f`.

You can also bring up specific services in detached mode:

```bash
docker compose up -d database cache
```

This is particularly useful for starting infrastructure services that other services depend on.

## Secure Communication

Security in container communication is critical, especially in production environments. Docker Compose provides several mechanisms to implement secure communication patterns.

### Managing Secrets

Docker has built-in support for secrets management:

```yaml
services:
  webapp:
    image: my-webapp
    secrets:
      - db_password
      - ssl_cert

secrets:
  db_password:
    file: ./secrets/db_password.txt
  ssl_cert:
    file: ./secrets/ssl_cert.pem
```

Secrets are mounted at `/run/secrets/<secret_name>` inside the container, keeping sensitive data out of environment variables or config files.

For development environments, you can also use environment variables with a secrets-like pattern:

```yaml
services:
  webapp:
    environment:
      - DB_PASSWORD_FILE=/run/secrets/db_password
    volumes:
      - ./dev-secrets:/run/secrets
```

### TLS Between Services

For production environments, you may want to encrypt traffic between services using TLS:

```yaml
services:
  api:
    volumes:
      - ./certs:/etc/certs
    environment:
      - CERT_PATH=/etc/certs/api.crt
      - KEY_PATH=/etc/certs/api.key
      - CLIENT_CA_PATH=/etc/certs/ca.crt
```

You'd then configure your application to use these certificates for both server and client authentication, implementing mutual TLS (mTLS) for secure service-to-service communication.

Docker also supports network encryption at the swarm level, but that's beyond the scope of Docker Compose and enters the realm of Docker Swarm orchestration.

## Hands-on Demo: Create a Robust Microservices Network with Health Checks and Secrets

Let's create a robust microservices architecture with proper networking, health checks, and secrets management. Our demo will include:
- An API gateway
- Two microservices
- A database
- A Redis cache

First, let's set up our project structure:

```
microservices-demo/
├── docker-compose.yml
├── gateway/
│   ├── Dockerfile
│   ├── server.js
│   └── package.json
├── service-a/
│   ├── Dockerfile
│   ├── server.js
│   └── package.json
├── service-b/
│   ├── Dockerfile
│   ├── server.js
│   └── package.json
└── secrets/
    ├── db_password.txt
    └── api_key.txt
```

Now, let's create our Docker Compose configuration:

```yaml
# docker-compose.yml
services:
  gateway:
    build: ./gateway
    ports:
      - "3000:3000"
    networks:
      - frontend
      - services
    depends_on:
      redis:
        condition: service_healthy
      service-a:
        condition: service_healthy
      service-b:
        condition: service_healthy
    environment:
      - SERVICE_A_URL=http://service-a:4000
      - SERVICE_B_URL=http://service-b:5000
      - REDIS_URL=redis://redis:6379
    secrets:
      - api_key
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  service-a:
    build: ./service-a
    networks:
      - services
      - database
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB_HOST=postgres
      - DB_USER=postgres
      - DB_NAME=service_a
      - DB_PASSWORD_FILE=/run/secrets/db_password
    secrets:
      - db_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  service-b:
    build: ./service-b
    networks:
      - services
      - database
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB_HOST=postgres
      - DB_USER=postgres
      - DB_NAME=service_b
      - DB_PASSWORD_FILE=/run/secrets/db_password
    secrets:
      - db_password
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - database
    environment:
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
      - POSTGRES_USER=postgres
    secrets:
      - db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    volumes:
      - redis-data:/data
    networks:
      - services
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

networks:
  frontend:
    driver: bridge
  services:
    driver: bridge
  database:
    driver: bridge
    internal: true

volumes:
  postgres-data:
  redis-data:

secrets:
  db_password:
    file: ./secrets/db_password.txt
  api_key:
    file: ./secrets/api_key.txt
```

Now let's create simplified Node.js services for our demo:

**Gateway package.json:**
```json
{
  "name": "gateway",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "axios": "^1.6.0",
    "redis": "^4.6.0"
  }
}
```

**Gateway (gateway/server.js):**
```javascript
const express = require('express');
const axios = require('axios');
const redis = require('redis');
const fs = require('fs');
const app = express();
const port = 3000;

// Read API key from Docker secret
const apiKey = fs.readFileSync('/run/secrets/api_key', 'utf8').trim();

// Create Redis client
const redisClient = redis.createClient({
  url: process.env.REDIS_URL
});

redisClient.connect().catch(console.error);

app.get('/health', (req, res) => {
  res.status(200).send('OK');
});

// Validate API key middleware
const validateApiKey = (req, res, next) => {
  const providedKey = req.header('X-API-Key');
  if (providedKey && providedKey === apiKey) {
    next();
  } else {
    res.status(401).send('Unauthorized');
  }
};

app.use(validateApiKey);

// Proxy requests to service-a
app.get('/service-a/:resource', async (req, res) => {
  try {
    // Check cache first
    const cacheKey = `service-a:${req.params.resource}`;
    const cachedData = await redisClient.get(cacheKey);
    
    if (cachedData) {
      return res.json(JSON.parse(cachedData));
    }
    
    // Forward request to service-a
    const response = await axios.get(`${process.env.SERVICE_A_URL}/${req.params.resource}`);
    
    // Cache the result
    await redisClient.set(cacheKey, JSON.stringify(response.data), {
      EX: 60 // Expire after 60 seconds
    });
    
    res.json(response.data);
  } catch (error) {
    res.status(500).send('Error proxying to service-a');
  }
});

// Proxy requests to service-b
app.get('/service-b/:resource', async (req, res) => {
  try {
    const response = await axios.get(`${process.env.SERVICE_B_URL}/${req.params.resource}`);
    res.json(response.data);
  } catch (error) {
    res.status(500).send('Error proxying to service-b');
  }
});

app.listen(port, () => {
  console.log(`Gateway listening at http://localhost:${port}`);
});
```

**Service A package.json:**
```json
{
  "name": "service-a",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.0"
  }
}
```

**Service A (service-a/server.js):**
```javascript
const express = require('express');
const { Pool } = require('pg');
const fs = require('fs');
const app = express();
const port = 4000;

// Read DB password from Docker secret
const dbPassword = fs.readFileSync('/run/secrets/db_password', 'utf8').trim();

// Configure PostgreSQL connection
const pool = new Pool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  database: process.env.DB_NAME || 'postgres',
  password: dbPassword,
  port: 5432,
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.status(200).send('OK');
  } catch (error) {
    res.status(500).send('Not healthy');
  }
});

// Sample data endpoint
app.get('/users', async (req, res) => {
  try {
    const result = await pool.query('SELECT * FROM users');
    res.json(result.rows);
  } catch (error) {
    res.status(500).send('Database error');
  }
});

// Initialize database on startup
const initDatabase = async () => {
  try {
    await pool.query(`
      CREATE TABLE IF NOT EXISTS users (
        id SERIAL PRIMARY KEY,
        name TEXT NOT NULL,
        email TEXT UNIQUE NOT NULL
      )
    `);
    
    // Insert sample data if table is empty
    const count = await pool.query('SELECT COUNT(*) FROM users');
    if (parseInt(count.rows[0].count) === 0) {
      await pool.query(`
        INSERT INTO users (name, email) VALUES
        ('Alice', 'alice@example.com'),
        ('Bob', 'bob@example.com')
      `);
    }
    
    console.log('Database initialized');
  } catch (error) {
    console.error('Database initialization failed:', error);
  }
};

app.listen(port, () => {
  console.log(`Service A listening at http://localhost:${port}`);
  initDatabase();
});
```

**Service B package.json:**
```json
{
  "name": "service-b",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.0"
  }
}
```

**Service B (service-b/server.js):**
```javascript
const express = require('express');
const { Pool } = require('pg');
const fs = require('fs');
const app = express();
const port = 5000;

// Read DB password from Docker secret
const dbPassword = fs.readFileSync('/run/secrets/db_password', 'utf8').trim();

// Configure PostgreSQL connection
const pool = new Pool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  database: process.env.DB_NAME || 'postgres',
  password: dbPassword,
  port: 5432,
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.status(200).send('OK');
  } catch (error) {
    res.status(500).send('Not healthy');
  }
});

// Sample data endpoint
app.get('/products', async (req, res) => {
  try {
    const result = await pool.query('SELECT * FROM products');
    res.json(result.rows);
  } catch (error) {
    res.status(500).send('Database error');
  }
});

// Initialize database on startup
const initDatabase = async () => {
  try {
    await pool.query(`
      CREATE TABLE IF NOT EXISTS products (
        id SERIAL PRIMARY KEY,
        name TEXT NOT NULL,
        price NUMERIC NOT NULL
      )
    `);
    
    // Insert sample data if table is empty
    const count = await pool.query('SELECT COUNT(*) FROM products');
    if (parseInt(count.rows[0].count) === 0) {
      await pool.query(`
        INSERT INTO products (name, price) VALUES
        ('Laptop', 1200),
        ('Phone', 800),
        ('Headphones', 150)
      `);
    }
    
    console.log('Database initialized');
  } catch (error) {
    console.error('Database initialization failed:', error);
  }
};

app.listen(port, () => {
  console.log(`Service B listening at http://localhost:${port}`);
  initDatabase();
});
```

**Dockerfile (for all Node.js services):**

```dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package.json .
RUN npm install

COPY server.js .

# Install wget for health checks
RUN apk add --no-cache wget

EXPOSE 3000

CMD ["node", "server.js"]
```

**Secret Files:**

Create the following files with secure content:
- `secrets/db_password.txt` - A strong database password
- `secrets/api_key.txt` - A secure API key for accessing the gateway

### Running the Demo

1. Start the entire stack in detached mode:
   ```bash
   docker compose up -d
   ```

2. Monitor the health status as services start up:
   ```bash
   docker compose ps
   ```

3. Once all services are healthy, test the API gateway:
   ```bash
   curl -H "X-API-Key: $(cat secrets/api_key.txt)" http://localhost:3000/service-a/users
   curl -H "X-API-Key: $(cat secrets/api_key.txt)" http://localhost:3000/service-b/products
   ```

This demo demonstrates several important concepts:
- Multiple isolated networks for security
- Health checks for proper dependency management
- Secret management for sensitive information
- Caching with Redis
- API gateway pattern with authentication

## Key Takeaways

1. **Network Segmentation**: Use custom networks to isolate different parts of your application and improve security.

2. **Health Checks**: Implement proper health checks to ensure services are actually ready before dependent services start.

3. **Secret Management**: Use Docker secrets to handle sensitive information securely, keeping it out of environment variables and config files.

4. **Restart Policies**: Configure appropriate restart policies to ensure your services recover from failures automatically.

5. **Service Discovery**: Leverage Docker Compose's built-in DNS resolution for easy service-to-service communication.

These advanced Docker Compose techniques help bridge the gap between simple container setups and full-scale container orchestration systems like Kubernetes, making it an excellent tool for both development and small to medium production deployments.
================================================================
# Docker Compose - Intermediate Level: Lesson 14

In this lesson, we'll explore how to effectively manage multi-container applications in production environments. As containerized applications move from development to production, different challenges arise related to deployment, configuration management, performance monitoring, and service orchestration.

## Deploying with docker-compose up -d --build

When deploying updates to a containerized application in production, you need a streamlined approach to rebuild images and restart services with minimal downtime. The `docker-compose up -d --build` command combines several operations into one efficient workflow:

```bash
docker-compose up -d --build
```

This command:
1. Builds or rebuilds all service images that have a `build` directive in your compose file
2. Starts all services defined in your compose file
3. Runs in detached mode (`-d`), meaning containers run in the background
4. Creates or recreates only containers whose configuration or image has changed

This approach is particularly useful when you've made changes to your application code and need to deploy those changes without disrupting other services. Docker Compose is smart enough to only rebuild and restart what's necessary.

For a more controlled deployment process, especially in production environments, you might want to split these steps:

```bash
# Build or rebuild images
docker-compose build

# Bring everything up in detached mode
docker-compose up -d
```

This separation allows you to verify that the build process completed successfully before actually deploying the changes.

## Handling Configuration Updates

When you need to update configurations or pull new versions of images, a common pattern is:

```bash
# Pull the latest versions of all images
docker-compose pull

# Apply the updates
docker-compose up -d
```

This sequence ensures you have the latest versions of all your images before redeploying. Docker Compose will only recreate containers for services whose images have changed.

For more controlled updates, especially when updating critical services, you might want to update services individually:

```bash
# Pull the latest version of a specific service
docker-compose pull web-service

# Update just that service
docker-compose up -d web-service
```

When handling configuration changes stored in environment files, you might need to reload the compose file to pick up these changes:

```bash
# After updating your .env file
docker-compose down
docker-compose up -d
```

Or for more targeted updates that don't require a full restart:

```bash
docker-compose up -d --force-recreate --no-deps web-service
```

This recreation ensures the service picks up new environment variables without stopping dependent services.

## Managing Logs & Monitoring Performance

### Resource Limits

In production environments, it's crucial to set resource limits to prevent any single container from consuming all available system resources:

```yaml
version: '3.8'

services:
  web-app:
    image: web-app:latest
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
```

**Note:** The `deploy` section only works with Docker Swarm mode. For standalone Docker Compose, use:

```yaml
version: '3.8'

services:
  web-app:
    image: web-app:latest
    mem_limit: 512m
    cpus: 0.5
    mem_reservation: 256m
```

These limits ensure that:
- The container can't use more than 50% of a CPU core
- The container can't consume more than 512MB of memory
- The container has a guaranteed reservation of 256MB of memory

Setting appropriate resource limits helps prevent resource contention and improves overall system stability, especially when running multiple services on the same host.

### Log Management

For proper log management in production, you can configure Docker's logging drivers:

```yaml
services:
  web-app:
    image: web-app:latest
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

This configuration:
- Uses the JSON file logging driver
- Limits log files to 10MB in size
- Keeps a maximum of 3 log files before rotating

For more advanced setups, you might want to send logs to centralized logging systems like ELK (Elasticsearch, Logstash, Kibana) or use the `fluentd` or `syslog` drivers.

## Using depends_on vs. Health Checks for Stability

Both `depends_on` and health checks help manage service dependencies, but they serve different purposes:

### depends_on

The `depends_on` option ensures services start in the correct order:

```yaml
services:
  web-app:
    image: web-app:latest
    depends_on:
      - database
      - redis
  
  database:
    image: postgres:latest
  
  redis:
    image: redis:latest
```

This configuration guarantees that:
- The `database` and `redis` services start before the `web-app` service
- When stopping services, Docker Compose stops them in reverse order

While `depends_on` handles startup order, it doesn't verify if services are actually ready to accept connections—it only ensures they've started. This can lead to application errors if your app tries to connect to a database that isn't yet ready to accept connections.

### Health Checks

Health checks provide a more robust way to handle dependencies by verifying that services are actually ready:

**Note:** The `condition: service_healthy` syntax requires Docker Compose file format version 2.1 or higher. For version 3.x files used with standalone Docker Compose (not Swarm), this feature is not available. Instead, implement waiting logic in your application code.

For Docker Compose v2.1:
```yaml
version: '2.1'

services:
  web-app:
    image: web-app:latest
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
  
  database:
    image: postgres:latest
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  redis:
    image: redis:latest
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
```

For Docker Compose v3.x (recommended approach):
```yaml
version: '3.8'

services:
  web-app:
    image: web-app:latest
    depends_on:
      - database
      - redis
    # Implement wait logic in your application or use an init container
  
  database:
    image: postgres:latest
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  redis:
    image: redis:latest
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
```

## Optimizing Startup Order of Services

Beyond the basic `depends_on` and health checks, complex applications often need more nuanced control over startup processes:

### Staged Startup

For applications with many services, a staged startup approach can reduce resource contention:

```yaml
version: '3.8'

services:
  # Infrastructure tier
  database:
    image: postgres:latest
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  redis:
    image: redis:latest
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
  
  # Application tier
  api-service:
    image: api-service:latest
    depends_on:
      - database
      - redis
  
  auth-service:
    image: auth-service:latest
    depends_on:
      - database
  
  # Frontend tier
  web-app:
    image: web-app:latest
    depends_on:
      - api-service
      - auth-service
```

This tiered approach:
- Groups services into logical tiers (infrastructure, application, frontend)
- Ensures each tier starts in the correct order
- Reduces the "thundering herd" problem when all services try to start simultaneously

### Startup Scripts

For more complex initialization, you can use entrypoint scripts to handle application-specific startup logic:

```yaml
services:
  api-service:
    image: api-service:latest
    entrypoint: ["/app/wait-for-it.sh", "database:5432", "--", "/app/startup.sh"]
    depends_on:
      - database
```

These scripts can:
- Check for dependent services using tools like `wait-for-it.sh` or `dockerize`
- Perform database migrations
- Load initial data
- Set up runtime configurations

This approach moves complex startup logic out of Docker Compose and into your application's domain, making it more maintainable and adaptable.

## Scaling Services and Orchestration Prep

As your application grows, you may need to scale certain services to handle increased load:

### Basic Scaling

Docker Compose supports basic horizontal scaling with the `--scale` flag:

```bash
docker-compose up -d --scale worker=3 --scale web-app=2
```

This command:
- Starts 3 instances of the `worker` service
- Starts 2 instances of the `web-app` service
- Maintains a single instance of any other services

For this to work properly, your services must be designed to run in parallel without port conflicts:

```yaml
services:
  web-app:
    image: web-app:latest
    ports:
      - "8080-8085:8080"  # Port range allows multiple instances
```

### Preparing for Orchestration

While Docker Compose can handle basic scaling, production applications often migrate to full orchestration platforms like Docker Swarm or Kubernetes. You can prepare your Compose files for this transition:

```yaml
version: '3.8'

services:
  web-app:
    image: web-app:latest
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.role == worker]
```

The `deploy` section includes configurations that will be used by Docker Swarm (but ignored by standalone Docker Compose):
- `replicas`: The number of containers to run
- `update_config`: How updates should be rolled out
- `restart_policy`: When containers should be restarted
- `placement`: Where containers should be deployed in a swarm

This dual-compatibility approach allows you to use the same configuration files across development (Docker Compose) and production (Docker Swarm) environments.

## Hands-on Demo: Monitoring Dashboard with Prometheus + Grafana

Let's set up a monitoring dashboard for your containerized applications using Prometheus and Grafana, complete with resource constraints to ensure stability.

First, create a project directory:

```bash
mkdir docker-monitoring
cd docker-monitoring
```

### Step 1: Create the Compose File

Create a `docker-compose.yml` file:

```yaml
version: '3.8'

services:
  # Application to monitor (example web service)
  web-app:
    image: nginx:alpine
    ports:
      - "8080:80"
    mem_limit: 256m
    cpus: 0.5
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/status.conf:/etc/nginx/conf.d/status.conf:ro
    depends_on:
      - prometheus
    networks:
      - monitoring_network

  # Node exporter to collect host metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    mem_limit: 128m
    cpus: 0.2
    networks:
      - monitoring_network

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    mem_limit: 512m
    cpus: 0.5
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin_password
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    mem_limit: 512m
    cpus: 0.5
    networks:
      - monitoring_network
    depends_on:
      - prometheus

networks:
  monitoring_network:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
```

### Step 2: Configure Prometheus

Create the Prometheus configuration file:

```bash
mkdir -p prometheus
```

Create `prometheus/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'nginx'
    static_configs:
      - targets: ['web-app:80']
    metrics_path: /nginx_status
    scrape_interval: 5s
```

### Step 3: Configure Nginx with Status Module

Create Nginx configuration files:

```bash
mkdir -p nginx
```

Create `nginx/nginx.conf`:

```nginx
user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    keepalive_timeout  65;

    include /etc/nginx/conf.d/*.conf;
}
```

Create `nginx/status.conf`:

```nginx
server {
    listen 80;
    server_name localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }

    location /nginx_status {
        stub_status on;
        access_log off;
        allow all;
    }

    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

### Step 4: Configure Grafana Dashboards

Set up Grafana provisioning:

```bash
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards
```

Create `grafana/provisioning/datasources/datasource.yml`:

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
```

Create `grafana/provisioning/dashboards/dashboard.yml`:

```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

### Step 5: Launch the Monitoring Stack

Start everything with:

```bash
docker-compose up -d
```

Monitor the startup process:

```bash
docker-compose logs -f
```

Access the monitoring interfaces:
- Grafana: http://localhost:3000 (login with admin/admin_password)
- Prometheus: http://localhost:9090
- Nginx: http://localhost:8080

### Step 6: Import a Dashboard

In Grafana, navigate to Dashboards > Import and use dashboard ID `1860` for Node Exporter Full.

This setup provides:
- Host metrics collection via Node Exporter
- NGINX metrics collection via stub_status
- Visualization with Grafana
- Resource constraints to ensure stability
- Health checks for service monitoring

## Mini Project: Video Streaming App with Scaling and Monitoring

Let's create a video streaming application that can scale to handle multiple requests and includes monitoring.

### Project Structure

```
video-streaming-app/
├── docker-compose.yml
├── nginx/
│   ├── nginx.conf
│   └── default.conf
├── api/
│   ├── Dockerfile
│   ├── package.json
│   └── server.js
├── encoder/
│   ├── Dockerfile
│   ├── package.json
│   ├── encoder.js
│   └── healthcheck.js
├── monitoring/
│   ├── prometheus.yml
│   └── grafana/
│       ├── datasources/
│       │   └── datasource.yml
│       └── dashboards/
│           └── dashboard.yml
└── frontend/
    ├── Dockerfile
    ├── package.json
    └── src/
        ├── App.js
        └── index.js
```

### Docker Compose Configuration

```yaml
version: '3.8'

services:
  # Nginx for serving videos and load balancing
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
      - video_data:/var/www/videos:ro
    networks:
      - frontend_net
      - streaming_net
    depends_on:
      - api
    mem_limit: 256m
    cpus: 0.5
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # API for video metadata and upload
  api:
    build: ./api
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/video_streaming
      - REDIS_URL=redis://redis:6379
      - PORT=3000
    volumes:
      - video_data:/app/videos
    networks:
      - streaming_net
      - backend_net
    depends_on:
      - mongodb
      - redis
    mem_limit: 512m
    cpus: 0.5
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Video Encoder Service
  encoder:
    build: ./encoder
    volumes:
      - video_data:/app/videos
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://mongodb:27017/video_streaming
    networks:
      - backend_net
    depends_on:
      - redis
      - mongodb
    mem_limit: 1g
    cpus: 1.0
    healthcheck:
      test: ["CMD", "node", "/app/healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Frontend React App
  frontend:
    build: ./frontend
    ports:
      - "3001:80"
    networks:
      - frontend_net
    depends_on:
      - api
    mem_limit: 256m
    cpus: 0.25

  # MongoDB for metadata storage
  mongodb:
    image: mongo:4.4
    volumes:
      - mongodb_data:/data/db
    networks:
      - backend_net
    mem_limit: 512m
    cpus: 0.5
    healthcheck:
      test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s

  # Redis for job queue
  redis:
    image: redis:6-alpine
    networks:
      - backend_net
    mem_limit: 256m
    cpus: 0.25
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - monitoring_net
      - backend_net
      - streaming_net
    mem_limit: 512m
    cpus: 0.25
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    volumes:
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin_secret
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - monitoring_net
    depends_on:
      - prometheus
    mem_limit: 512m
    cpus: 0.25

networks:
  frontend_net:
    driver: bridge
  streaming_net:
    driver: bridge
  backend_net:
    driver: bridge
    internal: true
  monitoring_net:
    driver: bridge

volumes:
  video_data:
  mongodb_data:
  prometheus_data:
  grafana_data:
```

### Scaling the Application

To scale specific services:

```bash
# Scale the API service to handle more requests
docker-compose up -d --scale api=3

# Scale the encoder service for parallel processing
docker-compose up -d --scale encoder=2

# Scale both services
docker-compose up -d --scale api=3 --scale encoder=2
```

For load balancing scaled API services, update the Nginx configuration to include upstream configuration.

### Deployment Commands

Complete deployment workflow:

```bash
# Build and deploy
docker-compose build
docker-compose up -d

# Update a specific service
docker-compose build api
docker-compose up -d --no-deps api

# Scale services
docker-compose up -d --scale encoder=3

# Monitor logs
docker-compose logs -f api encoder

# Clean shutdown
docker-compose down --volumes
```

This corrected version addresses several key issues:

1. **Resource Limits**: Fixed the `deploy` section usage and provided alternatives for standalone Docker Compose
2. **Health Check Dependencies**: Clarified the version compatibility issues and provided working alternatives
3. **Prometheus Configuration**: Fixed the metrics path for Nginx
4. **Nginx Configuration**: Added proper health endpoint and status configuration
5. **Project Structure**: Completed the file structure and added missing files
6. **Scaling Commands**: Added proper scaling examples and deployment workflows
7. **Network Configuration**: Ensured proper network connectivity between services

The document now provides accurate, working examples that can be implemented in real environments.